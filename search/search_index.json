{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#home","title":"Home","text":"<p>Calkit makes it easy to create \"single button\" reproducible research projects.</p> <p>Instead of a loosely related collection of files and manual instructions, turn your project into a version-controlled, self-contained \"calculation kit,\" tying together all phases or stages of the project: data collection, analysis, visualization, and writing, each of which can make use of the latest and greatest computational tools and languages. In other words, you, your collaborators, and readers will be able to go from raw data to research article with a single command, improving efficiency via faster iteration cycle time, reducing the likelihood of mistakes, and allowing others to more effectively build upon your work.</p> <p>Calkit makes this level of automation possible without extensive software engineering expertise by providing a project framework and toolset that unifies and simplifies the use of powerful enabling technologies like Git, DVC, Conda, Docker, and more, while guiding users away from common reproducibility pitfalls.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>A declarative pipeline that forces users to define an environment   for every stage, so long lists of instructions in a README and   \"but it works on my machine\" are things of the past.</li> <li>A CLI to run the project's pipeline to verify it's reproducible,   regenerating outputs as needed and   ensuring all   computational environments   (e.g., Conda,   Docker, uv, Julia)   match their specification.</li> <li>A schema to store structured metadata describing the   project's important outputs (in its <code>calkit.yaml</code> file)   and how they are created   (its computational environments and pipeline).</li> <li>A command line interface (CLI) to simplify keeping code, text, and larger   data files backed up in the same project repo using both   Git and DVC.</li> <li>A complementary self-hostable and GitHub-integrated   cloud system   to facilitate backup, collaboration,   and sharing throughout the entire research lifecycle.</li> <li>Overleaf integration, so code,   data, and LaTeX documents can all live in the same repo and be part of a   single pipeline (no more manual uploads!)</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>See installation.</p>"},{"location":"apps/","title":"Apps","text":""},{"location":"apps/#apps","title":"Apps","text":"<p>A project can define an app to enable users to interact with and/or make predictions from the results.</p> <p>See this project for an example, which creates an app with marimo and embeds it from HF Spaces.</p>"},{"location":"calculations/","title":"Calculations","text":""},{"location":"calculations/#calculations","title":"Calculations","text":"<p>One of the primary purposes doing research is to create knowledge that makes it possible to calculate predictions in order to make decisions. Calkit makes it possible to define such calculations for a project. For example:</p> <pre><code>calculations:\n  force:\n    name: Calculate force\n    description: This calculates force.\n    kind: formula\n    params:\n      formula: y = 0.55 * x + 545\n    inputs:\n      - name: x\n        description: This is the input variable.\n        dtype: float\n    output:\n      name: y\n      description: This is the output.\n      dtype: int\n      template: The result is {y:.1f}!\n</code></pre> <p>This calculation uses a very basic formula that computes an output from one input. It can be executed with:</p> <pre><code>calkit calc force --input x=122\n</code></pre> <p>Since the output has a <code>template</code> defined, Calkit will print a formatted result unless the <code>--no-format</code> option is specified.</p> <pre><code>The result is 612.0!\n</code></pre> <p>The above example is trivial, but the future vision is to enable all kinds of calculations to be defined, and to have these hosted and executed on the project's homepage on calkit.io such that consumers of your research can make use of it directly.</p> <p>Alternatively, for complex calculations, an interactive app can be defined for the project.</p>"},{"location":"calkit-yaml/","title":"The calkit.yaml file","text":""},{"location":"calkit-yaml/#the-calkityaml-file","title":"The <code>calkit.yaml</code> file","text":"<p>The <code>calkit.yaml</code> file serves as a small \"database\" for the project's important metadata, which includes its:</p> <ul> <li>Global or system-level dependencies or requirements   (applications, environmental variables, or other configuration steps)</li> <li>Questions the project seeks to answer</li> <li>Environments</li> <li>The pipeline</li> <li>Datasets</li> <li>Figures</li> <li>Publications (journal articles, conference papers, presentations, posters)</li> <li>Procedures</li> <li>References</li> <li>Subprojects (smaller projects executed as part of the main project)</li> <li>Calculations (ways to make predictions with the results)</li> <li>App (a way to allow users to interact with the results)</li> </ul> <p>Objects can be imported from other projects, which produces a chain of reference to allow tracking reuse and reduce redundant storage.</p>"},{"location":"calkit-yaml/#showcase","title":"Showcase","text":"<p>The project showcase is a list of elements that best represent the project, shown on the project homepage on the Calkit Cloud web app. For example:</p> <pre><code>showcase:\n  - text: Here is some text.\n  - figure: figures/my-figure.png\n  - text: There is a figure above.\n  - markdown: \"### This is a Markdown heading\"\n  - publication: paper/paper.pdf\n</code></pre> <p>This project has a showcase that includes Plotly figures saved as JSON, which render interactively.</p>"},{"location":"cli-reference/","title":"CLI reference","text":""},{"location":"cli-reference/#cli-reference","title":"CLI reference","text":"<p>Use <code>calkit --help</code> to see all the available commands and options.</p>"},{"location":"cloud-integration/","title":"Cloud integration","text":""},{"location":"cloud-integration/#cloud-integration","title":"Cloud integration","text":"<p>The Calkit Cloud (calkit.io) serves as a project management interface and a DVC remote for easily storing all versions of your data/code/figures/publications, interacting with your collaborators, reusing others' research artifacts, etc.</p> <p>After signing up, visit the settings page and create a token for use with the API. Then execute:</p> <pre><code>calkit config set token ${YOUR_TOKEN_HERE}\n</code></pre> <p>Like the rest of Calkit, the Cloud platform is free and open source, so you can host your own.</p>"},{"location":"cloud-integration/#using-dvc-remotes-other-than-calkitio","title":"Using DVC remotes other than calkit.io","text":"<p>It's possible to configure DVC to use a different remote storage location, e.g., an AWS S3 bucket. However, any artifacts stored externally will not be viewable on calkit.io, and permissions for these locations will need to be configured for each collaborator manually.</p>"},{"location":"datasets/","title":"Datasets","text":""},{"location":"datasets/#datasets","title":"Datasets","text":"<p>If your research project produces a dataset, you can indicate it as such to make it easy for others to reuse in their own project. These are listed in the <code>datasets</code> section of the project's <code>calkit.yaml</code> file.</p> <p>A dataset is identified by its path in the project repo, and this path can be a folder. For example:</p> <pre><code># In calkit.yaml\ndatasets:\n  - path: data/raw-data.csv\n    title: Raw data\n    description: This is the raw data.\n</code></pre>"},{"location":"datasets/#importing-or-reusing-a-dataset-from-another-project","title":"Importing or reusing a dataset from another project","text":"<p>A dataset can be imported with the CLI like:</p> <pre><code>calkit import dataset {owner_name}/{project_name}/{path} {local_path}\n</code></pre> <p>If this dataset is tracked with DVC, a new DVC remote will be created to pull it into your project. For datasets in the Calkit Cloud, this means the data will not be duplicated there.</p>"},{"location":"dependencies/","title":"Dependencies, configuration, and secrets","text":""},{"location":"dependencies/#dependencies-configuration-and-secrets","title":"Dependencies, configuration, and secrets","text":"<p>One major barrier to reproducibility is dependency management. If one relies too much on system-level dependencies, this can lead to reproducibility issues because a full system is hard to define with sufficient detail. Imagine trying to remember every change you've ever made to your computer and determining which will impact running your project!</p> <p>Our goal is then to minimize system-level dependencies and configuration as much as possible, and what remains should be generally applicable to many projects, e.g., Docker, uv, and of course Calkit itself. Conversely, relying on system-wide installations of things like Python packages is a bad idea. For software libraries and tools more specific to a project, use environments.</p> <p>Dependencies can be declared in a project's <code>calkit.yaml</code> file as a list in the <code>dependencies</code> section, and these will be checked before running the pipeline when <code>calkit run</code> is called. This way, when someone else tries to run your project, they will be notified and can fix the issue before trying again, which is more convenient than telling them to run through a list of setup steps in a README.</p> <p>Dependencies can be apps or environmental variables, the latter being useful for configuration of a project that needs to be unique on each user's machine, which can also be used to avoid committing secrets to the repo.</p> <p>The example below, taken from this project shows both a unique configuration variable (<code>STRAVA_CLIENT_ID</code>) and a secret (<code>STRAVA_CLIENT_SECRET</code>) that allow a different user to use copy and reuse the project without changing anything.</p> <pre><code>dependencies:\n  - docker\n  - name: STRAVA_CLIENT_ID\n    kind: env-var\n    notes: &gt;\n      The STRAVA_CLIENT_ID and STRAVA_CLIENT_SECRET environmental\n      variables can be set in the .env file after creating a Strava\n      application at https://www.strava.com/settings/api\n  - name: STRAVA_CLIENT_SECRET\n    kind: env-var\n</code></pre> <p>As we can see in the notes for <code>STRAVA_CLIENT_ID</code>, a <code>.env</code> file, which is kept out of version control, can be used to define these variables. <code>calkit set-env-var</code> can be used as a shortcut to set one of these in lieu of directly editing <code>.env</code>. <code>calkit check env-vars</code> can also be run to check for missing variables, prompting the user for their values and setting them in <code>.env</code>.</p> <p>Additional (non-secret) environmental variables can be set at the project level in <code>calkit.yaml</code> in the <code>env_vars</code> map:</p> <pre><code>env_vars:\n  MY_ENV_VAR_NAME: the-value-here\n  ANOTHER_VAR: another value\n</code></pre> <p>These will then be set for calls to the <code>run</code> and <code>xenv</code> commands.</p>"},{"location":"environments/","title":"Environments","text":""},{"location":"environments/#environments","title":"Environments","text":"<p>A computational environment describes the necessary conditions for code to run properly. Ensuring that every stage in your pipeline is run within a defined environment is a great way to improve reproducibility. There are many different environment management tools out there to choose from, and Calkit attempts to provide a similar interface for all of them. Calkit also attempts to enforce their usage in such a way that all important information about the environment is captured locally in the project in so-called \"lock files.\" This way, the project can be moved to other machines without needing to worry about manually installing packages.</p> <p>Calkit provides a means for defining or declaring environments inside a project's <code>calkit.yaml</code> file. There is also a command line utility <code>calkit xenv</code> for executing a command in one of these, which ensures that the environment matches its specification before execution.</p>"},{"location":"environments/#environment-types-and-definitions","title":"Environment types and definitions","text":"<p>Calkit supports the following environment types:</p> <ul> <li>Docker</li> <li>Conda</li> <li><code>venv</code>   (included in the Python standard library)</li> <li><code>uv</code> (both <code>venv</code> and project-based)</li> <li>Pixi</li> <li><code>renv</code></li> <li>Julia</li> <li>MATLAB</li> <li>SLURM</li> <li><code>ssh</code></li> </ul> <p>Environment definitions live in the project's <code>calkit.yaml</code> file in the <code>environments</code> section. Most environments will have a <code>path</code> property pointing to a file that lists the necessary dependencies--the \"spec.\" For example, a Python virtual environment or \"venv\" can be defined as a simple list of dependencies in a <code>requirements.txt</code> file, which might look like:</p> <pre><code>pandas&gt;=2\npolars==0.17.1\nmatplotlib\n</code></pre>"},{"location":"environments/#automatic-detection","title":"Automatic detection","text":"<p>For common environment types, Calkit will register a new environment upon its first use with <code>calkit xenv</code>. For example, if you run:</p> <pre><code>calkit xenv python scripts/run.py\n</code></pre> <p>Calkit will attempt to find an environment spec, create the environment, save it in <code>calkit.yaml</code>, export a lock file, and run the command in that environment. If there are multiple env specs, e.g., <code>requirements.txt</code> and <code>environment.yml</code>, you can provide the path, e.g.,</p> <pre><code>calkit xenv -p environment.yml python scripts/run.py\n</code></pre> <p>and Calkit will use that one to create the environment and save the path in <code>calkit.yaml</code>.</p>"},{"location":"environments/#checking-syncing-and-executing","title":"Checking, syncing, and executing","text":"<p>An environment can be checked that it matches its specification with:</p> <pre><code>calkit check env --name {env-name}\n</code></pre> <p>This will produce a \"lock file\" (inside the project's <code>.calkit/env-locks</code> directory if the environment manager doesn't export lock files by default), which uniquely identifies the actual environment that was created to help diagnose reproducibility issues down the road.</p> <p>A command can be executed in an environment with:</p> <pre><code>calkit xenv --name {env-name} -- {command}\n</code></pre> <p>Before the command is executed, Calkit will check that the environment matches its specification, and if it needs to be updated, that will be done before execution.</p> <p>All project environments can be checked at once with:</p> <pre><code>calkit check envs\n</code></pre>"},{"location":"environments/#choosing-an-environment-type","title":"Choosing an environment type","text":"<p>So which type of environment should you use? The short answer is: any. Any environment is better than none, where none means installing dependencies in the global host machine environment. If you want the long answer, keep reading.</p> <p>Docker is probably the most reproducible out of any environment type, since a Docker image includes information about the operating system. If it's convenient, e.g., if an image already contains all the necessary dependencies, go with a Docker environment. However, in some cases Docker may be a bit heavier than necessary.</p> <p>If you're running Python code, a <code>uv-venv</code> environment is a good default choice. <code>uv</code> is very easy to install and very fast.</p> <p>If you have non-Python dependencies that depend on complex compiled binaries (as scientific and engineering oriented tooling often does) and a <code>uv-venv</code> can't be built on your machine, A Conda environment is a good choice. However, Pixi has access to the same packages and is a bit faster. It's sort of like <code>uv</code> for Conda packages, and is similarly very easy to install.</p> <p>If you're working on a machine for which you don't have control to install dependencies, or working as part of a team, a plain old Python <code>venv</code> could be the best option.</p> <p>Again, try not to get too hung up on the decision of which environment type to use. Try one and see how it goes. Calkit should make the experience similar for all types.</p>"},{"location":"environments/#examples","title":"Examples","text":"<p>Creating any type of environment from the Calkit CLI follows a similar pattern starting with <code>calkit new</code>. You can view the help output with <code>calkit new --help</code> and filter it down to environment-related commands with <code>calkit new --help | grep env</code>.</p>"},{"location":"environments/#docker","title":"Docker","text":"<p>A new Docker environment can be added to the project with <code>calkit new docker-env</code>. A Docker environment can use an existing image, e.g., from Docker Hub, or it can create a new image, e.g., from a <code>Dockerfile</code> stored in the project repo.</p> <p>Let's say you want to add an OpenFOAM environment to your project. This can be achieved with something like:</p> <pre><code>calkit new docker-env --image microfluidica/openfoam:2412 --name foam\n</code></pre> <p>Then you can run a command in that environment with:</p> <pre><code>calkit xenv -n foam -- icoFoam -help\n</code></pre> <p>You can similarly jump into an interactive <code>bash</code> terminal with:</p> <pre><code>calkit xenv -n foam bash\n</code></pre> <p>But what if there isn't an image out there that has everything you need already installed into it? In this case, you can define and build a new derived image in the project by using the <code>--from</code> parameter, optionally adding predefined \"layers\" to the image with <code>--add-layer</code>. This will produce a Dockerfile defining the image, and when that environment is run with <code>calkit xenv</code>, that image will be built and a lock file produced.</p> <p>For example, running:</p> <pre><code>calkit new docker-env \\\n    --from microfluidica/openfoam:2412 \\\n    --name foam2 \\\n    --add-layer miniforge\n</code></pre> <p>will create a Dockerfile in the project and add the environment named <code>foam2</code> to the <code>calkit.yaml</code> file. Calling <code>calkit xenv -n foam2 bash</code> will cause the image to be built and a lock file <code>Dockerfile-lock.json</code> to be created. Note that the Dockerfile path can be controlled with the <code>--path</code> option.</p> <p>You can go in and modify the Dockerfile, e.g., to add more installation commands, and another call to <code>calkit xenv -n foam2</code> will kick off a rebuild automatically, since the lock file will no longer match the Dockerfile.</p> <p>If you're copying local files into the Docker image, you can declare these dependencies in the environment definition so the content of those will be tracked as well:</p> <pre><code># In calkit.yaml\nenvironments:\n  foam2:\n    kind: docker\n    image: foam2\n    deps:\n      - src/mySolver.C\n</code></pre> <p>This highlights Calkit's declarative design philosophy. Simply declare the environment and use it in a pipeline stage and Calkit will ensure it is built and up to date. There is no need to think about building images as a separate step.</p>"},{"location":"environments/#uv","title":"uv","text":"<p>uv can create both project and venv virtual environments. Project environments are defined by a <code>pyproject.toml</code> file, while venv environments are defined by a <code>requirements.txt</code> file.</p> <p>To create a new uv project environment, inside a project directory run something like:</p> <pre><code>calkit new uv-env -n my-env \"polars&gt;=1.0\" matplotlib\n</code></pre> <p>By default, this will create a <code>pyproject.toml</code> file in <code>.calkit/envs/my-env/pyproject.toml</code>, but the path can be controlled with the <code>--path</code> option.</p> <p>To create a new uv venv, simply replace <code>uv-env</code> with <code>uv-venv</code> in the above command and a <code>requirements.txt</code> file will be created instead.</p> <pre><code>calkit xenv -n my-env python -c \"import matplotlib, print(matplotlib.__version__)\"\n</code></pre> <p>If you were to run something like:</p> <pre><code>calkit xenv -n my-env python -c \"import pandas, print(pandas.__version__)\"\n</code></pre> <p>it would fail, since <code>pandas</code> is not present in the spec file (<code>pyproject.toml</code> or <code>requirements.txt</code>). However, if you add it in there, calling the above command again will succeed because Calkit automatically checks or syncs the environment before execution.</p>"},{"location":"environments/#venv","title":"venv","text":"<p>A <code>venv</code> environment, which uses Python's built-in <code>venv</code> module, can be used nearly identically to the <code>uv</code> example above. Simply replace <code>uv-venv</code> with <code>venv</code> in the <code>calkit new</code> call.</p>"},{"location":"environments/#conda","title":"Conda","text":"<p>As you might expect, Conda environments again work nearly identically to <code>uv-venv</code> and <code>venv</code> environments.</p> <p>You can create a new Conda environment with something like:</p> <pre><code>calkit new conda-env -n my-conda-env numpy matplotlib --pip pandas\n</code></pre> <p>Note that in this case, we specified one package, <code>pandas</code>, to be installed from the Python Package Index (PyPI) with <code>pip</code> using the <code>--pip</code> option.</p> <p>The new Conda environment spec will be written to <code>environment.yml</code> by default, which can be controlled with the <code>--path</code> option.</p> <p>A prefix for the environment can be specified to keep all packages under the project directory, e.g., by adding <code>--prefix .conda-envs/my-conda-env</code>. If this option is omitted, the environment will become part of Conda's system-wide collection of environments with a name like <code>{project_name}-{env_name}</code>, where the project name is added to avoid conflicts.</p> <p>Similar to other environment types, any time a command is executed with <code>calkit xenv</code>, this environment will be checked and created or updated as necessary.</p> <p>Calling:</p> <pre><code>calkit xenv -n my-conda-env -- which python\n</code></pre> <p>will create it. If you add any dependencies to <code>environment.yml</code>, calling that same command will cause the environment to be rebuilt before execution, and an updated <code>environment-lock.yml</code> file will be created. Again this highlights Calkit's declarative design philosophy. Declare the environment and what command should be executed inside, and Calkit will handle the rest.</p>"},{"location":"environments/#julia","title":"Julia","text":"<p>Julia environments have paths that point to a <code>Project.toml</code> file. Creating a new Julia environment is similar to creating a Python environment:</p> <pre><code>calkit new julia-env \\\n    --name my-julia-env \\\n    --path ./envs/my-julia-env/Project.toml \\\n    --julia 1.11 \\\n    WaterLily \\\n    Makie\n</code></pre> <p>With Julia environments, it's possible to execute a command:</p> <pre><code>calkit xenv -n my-julia-env -- -e \"println(\\\"hello world\\\");\"\n</code></pre> <p>or a script:</p> <pre><code>calkit xenv -n my-julia-env -- my_julia_script.jl arg1 arg2\n</code></pre> <p>Running Julia this was will ensure the global environment is ignored, meaning you can be sure if it's successful on your machine, it will be successful on others.</p>"},{"location":"environments/#slurm","title":"SLURM","text":"<p>SLURM is a job scheduler commonly used for high performance computing (HPC). A SLURM environment can be defined in <code>calkit.yaml</code> as follows:</p> <pre><code>environments:\n  my-hpc-cluster:\n    kind: slurm\n    host: hpc.myinstitute.org\n</code></pre> <p>To run a script in a <code>slurm</code> environment, use the <code>sbatch</code> pipeline stage type.</p>"},{"location":"environments/#ssh","title":"SSH","text":"<p>It's possible to define a remote environment that uses <code>ssh</code> to connect and run commands, and <code>scp</code> to copy files back and forth. This could be useful, e.g., for running one or more pipeline stages on an HPC cluster, or simply offloading some work to a virtual machine in the cloud with specialized hardware like a more powerful GPU.</p> <p>It is assumed that dependencies on the remote machine are managed separately.</p> <p>An SSH environment defined in <code>calkit.yaml</code> looks like:</p> <pre><code>environments:\n  cluster:\n    kind: ssh\n    host: \"10.225.22.25\"\n    user: my-user-name\n    wdir: /home/my-user-name/calkit/example-ssh\n    key: ~/.ssh/id_ed25519\n    send_paths:\n      - script.sh\n    get_paths:\n      - results\n</code></pre> <p>In the example above, we define an environment called <code>cluster</code>, where we specify the host IP address, our username on that machine, the working directory, the path to an SSH key on our local machine (so we can connect without a password), which paths we want to send before executing commands, and which we want to copy back after they finish. Wildcards in paths are supported, so the entire directory could be copied if desired by specifying <code>*</code>.</p> <p>To register an SSH key with the host, use <code>ssh-copy-id</code>. For example:</p> <pre><code>ssh-copy-id -i ~/.ssh/id_ed25519 my-user-name@10.225.22.25\n</code></pre> <p>To execute a command in this environment, we can add a stage like this to our pipeline in <code>calkit.yaml</code>:</p> <pre><code>pipeline:\n  stages:\n    run-simulation:\n      kind: shell-script\n      script_path: script.sh\n      outputs:\n        - results\n</code></pre>"},{"location":"environments/#matlab","title":"MATLAB","text":"<p>Adding a MATLAB environment to a project will cause Calkit to automatically generate a Docker image based on its <code>version</code> and <code>products</code> attributes. A <code>MATLAB_LICENSE_SERVER</code> environmental variable must be set so the container can properly contact a license server. This can be done with:</p> <pre><code>calkit set-env-var MATLAB_LICENSE_SERVER &lt;XXXX@some.server.edu&gt;\n</code></pre> <p>Note that environmental variables set this way will be ignored by Git, and so will need to be set on each new machine on which the project is to be run.</p> <p>A MATLAB environment (and a pipeline stage that uses it) looks like:</p> <pre><code># In calkit.yaml\nenvironments:\n  my-matlab-2024b:\n    kind: matlab\n    version: R2024b\n    products:\n      - Simulink\n      - Global_Optimization_Toolbox\n      - Parallel_Computing_Toolbox\n\npipeline:\n  stages:\n    my-matlab-script:\n      kind: matlab-script\n      script_path: scripts/run_sim.m\n      environment: my-matlab-2024b\n      inputs:\n        - config/my-sim-config.json\n      outputs:\n        - results/sim-results.h5\n</code></pre>"},{"location":"environments/#pixi","title":"Pixi","text":"<p>Pixi environments typically have the path <code>pixi.toml</code>:</p> <pre><code>environments:\n  my-pixi:\n    kind: pixi\n    path: pixi.toml\n</code></pre>"},{"location":"environments/#r","title":"R","text":"<p>R environments can be managed with Conda, Pixi, or <code>renv</code> (recommended). The env spec path for an renv environment is typically a <code>DESCRIPTION</code> file.</p> <pre><code>environments:\n  r:\n    kind: renv\n    path: DESCRIPTION\n</code></pre> <p>The imports inside <code>DESCRIPTION</code> are used to create and sync the environment:</p> <pre><code>Package: CalkitProject\nVersion: 0.0.1\nTitle: Auto-generated R environment\nImports: tidyverse\n</code></pre>"},{"location":"examples/","title":"Examples","text":""},{"location":"examples/#examples","title":"Examples","text":"<p>Here are some example projects that use Calkit. If you have one you'd like to be included, please \u270f\ufe0f edit this page.</p>"},{"location":"examples/#naca-0012-2-d-rans-with-openfoam","title":"NACA 0012 2-D RANS with OpenFOAM","text":"<p>Project page | GitHub repo</p> <p>Features:</p> <ul> <li>A Docker environment for simulation</li> <li>A uv venv environment</li> <li>Interactive Plotly figures saved as JSON</li> <li>Interactive HTML figures generated with PyVista</li> <li>An interactive marimo app hosted on HF spaces for interacting with the   results</li> </ul>"},{"location":"examples/#a-basic-research-project","title":"A basic research project","text":"<p>Project page | GitHub repo</p> <p>Features:</p> <ul> <li>An automatically-managed Conda environment for data processing and   visualization</li> <li>A LaTeX publication built with a Docker container</li> <li>A dev container spec to enable editing and collaboration with GitHub   Codespaces</li> </ul>"},{"location":"examples/#matlab","title":"MATLAB","text":"<p>Project page | GitHub repo</p> <p>Features:</p> <ul> <li>Dependency checking before pipeline execution</li> <li>MATLAB scripts run in batch mode</li> </ul>"},{"location":"examples/#strava-analysis","title":"Strava analysis","text":"<p>Project page | GitHub repo</p> <p>Features:</p> <ul> <li>OAuth2 authentication with an external API</li> <li>Environmental variable dependencies</li> <li>A pipeline designed to be run periodically to accumulate new data</li> <li>A project showcase with interactive Plotly figures</li> <li>A uv project-based environment and dedicated Python package</li> </ul>"},{"location":"examples/#openfoam-rans-boundary-later-validation","title":"OpenFOAM RANS boundary later validation","text":"<p>Project page | GitHub repo</p> <p>Features:</p> <ul> <li>OpenFOAM simulations run in a Docker container</li> <li>A LaTeX document built with a Docker container</li> <li>A direct numerical simulation dataset for validation imported from a   different project, derived from the Johns Hopkins Turbulence Database</li> </ul>"},{"location":"examples/#ssh","title":"SSH","text":"<p>Project page | GitHub repo</p> <p>Features:</p> <ul> <li>An SSH environment for running a remote command over SSH and copying back   results to the local machine</li> </ul>"},{"location":"examples/#overleaf-integration","title":"Overleaf integration","text":"<p>Project page | GitHub repo</p> <p>Features:</p> <ul> <li>A publication linked to an Overleaf project, which syncs changes to the   text from Overleaf, and pushes figures generated locally to Overleaf.</li> </ul>"},{"location":"governance/","title":"Governance","text":""},{"location":"governance/#governance","title":"Governance","text":"<p>Calkit is a free, open-source, and openly-governed project. All issues and ideas are discussed publicly and are open to input from anyone. Because the project is still early-stage, the founder serves as the primary maintainer and decision maker. As the number of contributors grows, maintainers will be added to share review and release duties. The contributor's guide can be found here and our code of conduct can be found here.</p>"},{"location":"governance/#vision","title":"Vision","text":"<p>We envision a future where nearly every research article is delivered as part of a single-button reproducible compendium (or \"repro pack\"). That is, all primary artifacts like source code and raw data are available, and all secondary artifacts like figures and article PDFs can be produced with a single command.</p> <p></p> <p>We believe that this will increase the pace of collective knowledge creation by enabling faster:</p> <ol> <li>Discovery of errors.    Computational methods will be fully described and auditable.</li> <li>Replication.    Studies can be replicated by simply replacing the raw input data and    rerunning the pipeline.</li> <li>Extension.    Innovation in science largely occurs at the level of    hypotheses and conceptual models,    not in mundane pipeline construction and execution.    There is also a network effect as more types of single-button projects are    published,    making it easier to find something similar to a desired workflow to start    from and adapt to new questions.</li> </ol>"},{"location":"governance/#the-status-quo","title":"The status quo","text":"<p>The figure below from the PLOS Open Science Indicators dataset shows how far away we are from our target. Code sharing rates are only ~10%, and of what is shared, it's reasonable to assume only ~10% of that code will even run, never mind be included in a complete, automated pipeline.</p> <p></p> <p>Code sharing rates from PLOS Open Science Indicators.</p>"},{"location":"governance/#strategy","title":"Strategy","text":"<p>We believe one major hurdle preventing researchers from working reproducibly is the expectation that they become software engineering experts, choose and integrate multiple tools, and assemble a custom workflow. We want to provide a vertically-integrated, purpose-built, and user-friendly project format and toolset that reduces the required expertise and decision fatigue. That is not to say that policy, education, and support are not part of the solution\u2014they certainly are\u2014but we are focused on improving tooling and infrastructure.</p> <ul> <li>Path of least resistance:   Make it faster to work in a clean,   reproducible way than it is to work in an ad-hoc, disorganized way.</li> <li>Intuitive tooling:   Simplify the \"hard parts\" of modern scientific computing:   caching, version control, and environment management.   Instead of telling scientists to adapt software development tools for   their science, build the tools that do the adaptation.</li> <li>Bridging the gap:   Create a natural transition from interactive discovery (notebooks/shells)   to automated batch pipelines.</li> <li>Builder's pride:   Enable researchers to take pride in what they create so   they will be more likely to share their projects openly.</li> </ul>"},{"location":"governance/#objective-and-key-results-okrs","title":"Objective and key results (OKRs)","text":""},{"location":"governance/#2026-q1","title":"2026-Q1","text":"<ol> <li>Objective: Empower researchers to create and share single-button     reproducible research projects.<ol> <li>Key result: 5 researchers (excluding direct collaborators) create     single-button reproducible research projects this quarter.</li> <li>Key result: Of those who create projects, at least half share them     openly and cite them in submitted articles.</li> <li>Key result: At least half of those who create projects agree that they     would have been slower to finish (typically submit a paper) without     Calkit.</li> </ol> </li> </ol>"},{"location":"governance/#funding-and-sustainability","title":"Funding and sustainability","text":"<p>Calkit is committed to remaining free and open source forever. The project is sustained through a combination of:</p> <ul> <li>Volunteer contributions from the community.</li> <li>Institutional support through allocated work time.</li> <li>Calkit Cloud optional paid plans to help cover infrastructure costs   for the cloud storage and compute service hosted at   calkit.io.</li> </ul> <p>The cloud service operates on a freemium model: a generous free tier for most users, with paid options for those who need additional storage or compute resources. This helps ensure the service remains available and reliable without requiring payment for typical research projects.</p> <p>All Calkit software remains MIT-licensed and can be self-hosted and used with any compatible storage backend. In fact, we would prefer institutions host their own instance as part of a decentralized, federated network.</p>"},{"location":"help/","title":"Help and support","text":""},{"location":"help/#help-and-support","title":"Help and support","text":"<p>If you notice a bug or have suggestion for a new feature, use the issue tracker.</p> <p>To get in touch with the community, check out the discussion forum or the Discord server.</p> <p>If you want to get in touch directly, feel free to send an email.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#installation","title":"Installation","text":"<p>On Linux, macOS, or Windows Git Bash, install Calkit and uv (if not already installed) with:</p> <pre><code>curl -LsSf install.calkit.org | sh\n</code></pre> <p>Or with Windows Command Prompt or PowerShell:</p> <pre><code>powershell -ExecutionPolicy ByPass -c \"irm install-ps1.calkit.org | iex\"\n</code></pre> <p>If you already have uv installed, install Calkit with:</p> <pre><code>uv tool install calkit-python\n</code></pre> <p>You can also install with your system Python:</p> <pre><code>pip install calkit-python\n</code></pre> <p>To effectively use Calkit, you'll want to ensure Git is installed and properly configured. You may also want to install Docker, since that is the default method by which LaTeX environments are created. If you want to use the Calkit Cloud for collaboration and backup as a DVC remote, you can set up cloud integration.</p>"},{"location":"installation/#use-without-installing","title":"Use without installing","text":"<p>If you want to use Calkit without installing it, you can use uv's <code>uvx</code> command to run it directly:</p> <pre><code>uvx calk9 --help\n</code></pre>"},{"location":"installation/#calkit-assistant","title":"Calkit Assistant","text":"<p>For Windows users, the Calkit Assistant app is the easiest way to get everything set up and ready to work in VS Code, which can then be used as the primary app for working on all scientific or analytical computing projects.</p> <p></p>"},{"location":"jupyterlab/","title":"JupyterLab","text":""},{"location":"jupyterlab/#using-calkit-with-jupyterlab","title":"Using Calkit with JupyterLab","text":"<p>Calkit includes a JupyterLab extension for managing a project's environments and pipeline. Installing Calkit will install JupyterLab itself as well, so you can start it up with:</p> <pre><code>cd my-project-folder\ncalkit jupyter lab\n</code></pre> <p>With the extension, you won't need to remember to:</p> <ol> <li>Fully document the environment for a notebook with    <code>pip freeze</code> or <code>conda env export</code>.</li> <li>Ensure the correct kernel is being used when a notebook is opened.</li> <li>Ensure each notebook was run top-to-bottom any time its code or input    files (e.g., datasets) have change.</li> </ol> <p>Further, it will be obvious if any notebook outputs, e.g., figures, are out-of-date and require a rerun.</p> <p>When you first open a new notebook, you'll see badges in the toolbar showing that the notebook has no environment or pipeline stage defined.</p> <p></p> <p>Clicking on the environment badge will open a dialog allowing you to select or create an environment for the notebook. The environment editor will let you give the environment a name and select which packages and versions to include. There are \"package group\" shortcut buttons to quickly add common packages. For example, the PyData package group includes NumPy, Pandas, Matplotlib, SciPy, and more.</p> <p></p> <p>After assigning an environment to the notebook, the kernel will be switched automatically, and the badge will update to show the selected environment. Note that if you move the project to a different machine or share it with a collaborator, Calkit will recreate the environment automatically based on the defined packages.</p> <p>Next, click the pipeline badge to assign a pipeline stage to the notebook. You'll see options for which type of storage (Git, DVC, or none) to use to save the executed and HTML versions of the notebook when it's run as part of the pipeline.</p> <p></p> <p>If the notebook reads any files, you'll want to add those as inputs using the corresponding badge in the toolbar. The same can be done for outputs, or files written by the notebook. Defining these properly will allow Calkit to determine when a given output is \"stale\" and needs to be regenerated.</p> <p>The real magic happens when you have multiple notebooks in a project, where some notebooks depend on the outputs of others. Calkit will automatically determine the correct order to run the notebooks based on their inputs and outputs, and only rerun notebooks when their inputs have changed.</p> <p>For example, you could have a <code>collect-data</code> notebook, a <code>process-data</code> notebook, and a <code>plot-results</code> notebook. The <code>process-data</code> notebook would list the output of <code>collect-data</code> as an input, and <code>plot-results</code> would list the output of <code>process-data</code> as an input. If something changed about the <code>collect-data</code> notebook, Calkit would show the pipeline as out-of-date by turning the play button in the sidebar orange. Clicking the play button would then run all three notebooks in the correct order to regenerate all outputs. If only the <code>plot-results</code> notebook needed to be rerun, only that notebook would be executed. Notebook stages will also show up as stale if their environment has changed, ensuring that all code is always run with the correct dependencies.</p> <p></p> <p>After running the full pipeline, the play button will turn green, indicating that all notebooks and outputs are up-to-date.</p> <p></p>"},{"location":"local-server/","title":"Local server (GUI interaction)","text":""},{"location":"local-server/#local-server-for-gui-interaction","title":"Local server for GUI interaction","text":"<p>Calkit includes a local server for interacting with projects locally via the Calkit Cloud UI (calkit.io). It can be launched with:</p> <pre><code>calkit local-server\n</code></pre> <p>If you then navigate to the project homepage on calkit.io and visit the \"local machine\" tab, you'll be able to visually perform tasks like committing and ignoring files, running the pipeline, adding stages to the pipeline, and pushing to the cloud.</p> <p>This feature is used in the Microsoft Office tutorial.</p> <p></p> <p>The local server status widget on the Calkit Cloud UI.</p>"},{"location":"notebooks/","title":"Notebooks","text":""},{"location":"notebooks/#working-with-notebooks","title":"Working with notebooks","text":"<p>While working on a research project, Jupyter notebooks can be useful for prototyping and data exploration. If while working interactively in a notebook you get an output you like, e.g., a figure, it can be tempting to simply stop right there and copy/paste it into a research article. However, in order to keep the project reproducible, we need to be able to go from raw data to research article with a single command, which of course is not possible in the above scenario.</p> <p>This is the primary notebook use case Calkit is concerned with: generating evidence to back up conclusions or answers to research questions. There are other use cases that are out of scope like using notebooks to build documentation or interactive web apps for exploring results. For building apps (a different concept in a Calkit project), there are probably better tools out there, e.g., marimo, Dash, Voila, or Gradio.</p> <p>Here we'll talk about how to take advantage of the interactive nature of Jupyter notebooks while incorporating them into a reproducible workflow, avoiding some of the pitfalls that have caused a bit of a notebook reproducibility crisis. Returning to the \"one project, one command\" requirement, we can focus on three rules:</p> <ol> <li>The notebook must be kept in version control.    This happens naturally since any file included in a Calkit project is    kept in version control.    However, it's usually a good idea to exclude notebook output from    Git commits.    This can be done by installing <code>nbstripout</code> and running    <code>nbstripout --install</code> in the project directory.</li> <li>A notebook must run in one of the project's environments.</li> <li>Notebooks should be incorporated into the project's    pipeline.    It's fine to do some ad hoc work interactively to get the notebook    working properly, but    \"official\" outputs should be generated by calling <code>calkit run</code>.    This means notebooks need to be able to run from top-to-bottom with no    manual intervention. We'll see how below.</li> </ol> <p>If you use JupyterLab, check out the Calkit JupyterLab extension docs.</p>"},{"location":"notebooks/#creating-an-environment-for-a-notebook","title":"Creating an environment for a notebook","text":"<p>Assuming you want to run Python in the notebook, you can create an environment for it with <code>uv</code>, <code>venv</code>, <code>conda</code>, or <code>pixi</code>. For example, if we wanted to create a new <code>uv-venv</code> called <code>py</code> in our project, we can execute:</p> <pre><code>calkit new uv-venv \\\n    --name py \\\n    --prefix .venv \\\n    --python 3.13 \\\n    --path requirements.txt \\\n    jupyter \\\n    \"pandas&gt;=2\" \\\n    numpy \\\n    plotly \\\n    matplotlib \\\n    polars\n</code></pre> <p>You can then start JupyterLab in this environment with <code>calkit xenv -n py jupyter lab</code>.</p> <p>Note the environment only needs to be created once per project. If the project is cloned onto a new machine, the environment does not need to be recreated, since that will be done automatically when the project is run. Also note that it's totally fine and perhaps even preferable to create a new environment for each notebook, so long as they have different names, prefixes, and paths---there is no limit to the number of environments a project can use, and they can be of any type.</p>"},{"location":"notebooks/#adding-a-notebook-to-the-pipeline","title":"Adding a notebook to the pipeline","text":"<p>A notebook can be added to the pipeline either with <code>calkit new jupyter-notebook-stage</code> or by editing the project's <code>calkit.yaml</code> file directly. For example:</p> <pre><code># In calkit.yaml\nenvironments:\n  py:\n    kind: uv-venv\n    prefix: .venv\n    python: \"3.13\"\n    path: requirements.txt\npipeline:\n  stages:\n    my-notebook:\n      kind: jupyter-notebook\n      environment: py\n      notebook_path: notebooks/get-data.ipynb\n      inputs:\n        - config/my-params.json\n      outputs:\n        - data/raw/data.csv\n      html_storage: dvc\n      executed_ipynb_storage: null\n      cleaned_ipynb_storage: git\n# Optional: Add to project notebooks so they can be viewed on Calkit Cloud\nnotebooks:\n  - path: notebooks/get-data.ipynb\n    title: Get data\n    stage: my-notebook\n</code></pre> <p>For this example, we're declaring that the notebook should use the <code>py</code> environment, and that it will read an input file <code>config/my-params.json</code> and produce an output file <code>data/raw/data.csv</code>. These inputs and outputs will be tracked along with the notebook and environment content, to automatically determine if and when the notebook needs to be rerun. Outputs will also be kept in DVC by default so others can pull them down without bloating the Git repo. Output storage is configurable, however, e.g., if you'd like to keep smaller and/or text-based outputs in Git for simplicity's sake.</p> <p>Copies of the notebook with and without outputs will be generated as the notebook is executed, along with an HTML export of the latter. Storage for these outputs can be controlled with the <code>html_storage</code>, <code>executed_ipynb_storage</code>, <code>cleaned_ipynb_storage</code> properties, and they will live inside the project's <code>.calkit</code> subdirectory. The executed <code>.ipynb</code> can be rendered on GitHub or nbviewer.org, and the HTML can be viewed on calkit.io, the latter of which allows some level of interactivity, e.g., Plotly figures. The cleaned <code>.ipynb</code> can be useful for diffing with Git in cases where <code>nbstripout</code> is not activated.</p> <p>It's also possible to add a notebook to the pipeline inside a notebook with the <code>declare_notebook</code> function, which will update <code>calkit.yaml</code> automatically.</p> <pre><code>import calkit\n\ncalkit.declare_notebook(\n    path=\"notebooks/get-data.ipynb\",\n    stage_name=\"my-notebook\",\n    environment_name=\"py\",\n    inputs=[\"config/my-params.json\"],\n    outputs=[\"data/raw/data.csv\"],\n    html_storage=\"dvc\",\n    executed_ipynb_storage=None,\n    cleaned_ipynb_storage=\"git\",\n)\n</code></pre> <p>Note that for this to run properly <code>calkit-python</code> must be installed in the notebook's environment, which in this case is named <code>py</code> and whose packages are listed in <code>requirements.txt</code>. If we didn't include them when creating the environment, we can simply add <code>calkit-python</code> to the <code>requirements.txt</code> file and rerun <code>calkit xenv -n py jupyter lab</code>. The environment will be updated before starting JupyterLab.</p>"},{"location":"notebooks/#working-interactively","title":"Working interactively","text":"<p>The main advantage of Jupyter notebooks is the ability to work interactively, allowing us to quickly iterate on a smaller chunk of the process while the rest remains constant. For example, if you need to refine a figure, you can keep updating and running the cell that generates the figure, without needing to rerun the expensive cell above that generates or processes the data for it. In this case our notebook might look like this:</p> <pre><code>from some_package import run_data_processing\n\nresult = run_data_processing(param1=55)\n</code></pre> <pre><code>import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\nax.plot(result[\"x\"], result[\"y\"])\n</code></pre> <pre><code>fig.savefig(\"figures/my-plot.png\")\n</code></pre> <p>So, with a fresh Jupyter kernel we'll need to run cell 1 in order to generate <code>result</code> so we can iterate on cell 2 to get the plot looking the way we want it to. But what if <code>run_data_processing</code> takes minutes, hours, or even days, so therefore we don't want to run it every time we restart the notebook? Well, we can use the Calkit <code>%%stage</code> cell magic to automatically cache and retrieve the result.</p> <p>After adding a cell with:</p> <pre><code>%load_ext calkit.magics\n</code></pre> <p>the first cell can be turned into a pipeline stage by changing it to:</p> <pre><code>%%stage --name run-nb-proc --environment py --out result\n\nfrom some_package import run_data_processing\n\nresult = run_data_processing(param1=55)\n</code></pre> <p>In the magic command we're giving the cell a unique name, declaring which environment it should run in (<code>py</code> above, but it can be any environment in the project), and declaring an output from the cell that we want to be available to cells below.</p> <p>Now, the kernel can be restarted and we can use \"run all cells above\" when working on the figure, and we'll have <code>result</code> nearly instantaneously. <code>result</code> will also be versioned with DVC and pushed to the cloud by default, so our collaborators can also take advantage of the caching without bloating the Git repo. Execution as part of the project's pipeline will also take advantage of the caching and will not rerun data processing unless something about that cell's code or environment has changed.</p> <p>For a more in-depth look at using the <code>%%stage</code> cell magic, see this tutorial.</p>"},{"location":"notebooks/#parameterizing-notebooks","title":"Parameterizing notebooks","text":"<p>Thanks to Papermill, Calkit can run notebooks that have been parameterized, and executed versions will be saved with parameters in their names. To parameterize a notebook, first add a cell with the \"parameters\" tag and add your parameters there as variable declarations, e.g.,</p> <pre><code>param1 = 5\nparam2 = \"something\"\n</code></pre> <p>In JupyterLab, you can use the property inspector to edit cell tags:</p> <p></p> <p>In VS Code, the cell context menu can be used to mark the cell as parameters:</p> <p></p> <p>Then, in the Calkit pipeline, add <code>parameters</code> to the notebook stage:</p> <pre><code># In calkit.yaml\nenvironments:\n  main:\n    kind: uv-venv\n    path: requirements.txt\n    prefix: .venv\n    python: \"3.13\"\npipeline:\n  stages:\n    notebook-56-a:\n      kind: jupyter-notebook\n      notebook_path: notebook.ipynb\n      environment: main\n      parameters:\n        param1: 56\n        param2: a\n    notebook-58-b:\n      kind: jupyter-notebook\n      notebook_path: notebook.ipynb\n      environment: main\n      parameters:\n        param1: 58\n        param2: b\n</code></pre>"},{"location":"notebooks/#iterating-over-parameterized-notebooks","title":"Iterating over parameterized notebooks","text":"<p>The example below shows project-level parameters used to iterate over a notebook stage:</p> <pre><code>parameters:\n  param1:\n    - range:\n        start: 0\n        stop: 10\n        step: 2\n  param2:\n    - random-forest\n    - lightgbm\npipeline:\n  stages:\n    my-notebook-with-params:\n      kind: jupyter-notebook\n      environment: my-env\n      notebook_path: notebook.ipynb\n      iterate_over:\n        - arg_name: param1\n          values:\n            - parameter: param1 # Args and params can be named differently\n        - arg_name: param2\n          values:\n            - parameter: param2\n      parameters:\n        param1: \"{param1}\" # Notebook params can also be named differently\n        param2: \"{param2}\"\n      outputs:\n        - results/param1={param1}/param2={param2}/results.csv\n</code></pre> <p>For more information, see pipeline iteration.</p>"},{"location":"overleaf/","title":"Overleaf integration","text":""},{"location":"overleaf/#overleaf-integration","title":"Overleaf integration","text":"<p>Overleaf is a cloud-based web application designed for collaborating on LaTeX documents. It helps lower the barrier to entry as users don't need to get their local machine or a GitHub Codespace set up with Git, Docker, LaTeX, etc.</p> <p>One downside to using Overleaf is that it is intended only for writing, not general computing, e.g., data processing or figure generation, so it encourages treating writing as a separate phase or project. Any figures or tables created from automated scripts typically need to be manually uploaded to update the Overleaf document, which introduces complexity and a potential source of non-reproducibility, e.g., if this manual figure copying process is mistakenly omitted. It also makes it difficult to work offline.</p> <p>With Calkit it's possible to link an Overleaf project to a publication so you can use Overleaf for collaborating on the writing, without losing the ability to work more holistically on the project. Calkit can sync bidirectionally with Overleaf, ensuring edits propagate both directions, so users who prefer to work locally can do so. Calkit can also ensure files like figures are always sent from the local project (where they are generated) up to Overleaf, so the PDF output looks the same in either system.</p>"},{"location":"overleaf/#generating-and-storing-an-overleaf-token","title":"Generating and storing an Overleaf token","text":"<p>In order for Calkit to interact with Overleaf, you'll need to set a token in the config. To do this, visit the Overleaf user settings page and scroll down to the \"Your Git authentication tokens\" section. Generate a token, copy it, and then set it in your Calkit config with:</p> <pre><code>calkit config set overleaf_token {paste your token here}\n</code></pre>"},{"location":"overleaf/#importing-an-overleaf-project","title":"Importing an Overleaf project","text":"<p>To import an Overleaf project as a Calkit publication, use the <code>calkit import overleaf</code> command. For example:</p> <pre><code>calkit overleaf import \\\n    https://www.overleaf.com/project/68000059d42b134573cb2e35 \\\n    paper\n</code></pre> <p>This command will link a local project folder, in this case <code>paper</code>, to the Overleaf project, and always push the <code>paper/figures</code> folder, i.e., the figures will be one-way synced, whereas any other files will be synced bidirectionally.</p> <p>If necessary, this command will also create a TeXlive Docker environment and a build stage in the pipeline, which will build and cache the PDF upon calling <code>calkit run</code>.</p>"},{"location":"overleaf/#syncing-an-overleaf-project","title":"Syncing an Overleaf project","text":"<p>To sync a publication linked to an Overleaf project, simply call:</p> <pre><code>calkit overleaf sync\n</code></pre> <p>After syncing, you'll probably want to ensure the local PDF is up-to-date by calling <code>calkit run</code>, and if anything has changed, commit and push those changes to the cloud with <code>calkit save -am \"Run pipeline\"</code>.</p>"},{"location":"overleaf/#example","title":"Example","text":"<p>You can view an example project that uses Overleaf integration on GitHub and the Calkit Cloud. This project syncs the document text bidirectionally, and pushes figures up to Overleaf.</p>"},{"location":"overleaf/#merge-conflicts","title":"Merge conflicts","text":"<p>If the same lines are changed in a file in both the main project and the Overleaf project a \"merge conflict\" will occur. In this case, the text will need to be merged together manually. VS Code has a built-in merge conflict resolution tool, but there are many to choose from.</p> <p>In the file, e.g., <code>paper.tex</code>, you'll see something like:</p> <pre><code>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\nI made this edit locally. It's pretty great.\n=======\nI made this edit on Overleaf. It's great.\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; &lt;commit-id-of-patch&gt;\n</code></pre> <p>After merging the two chunks together and deleting the lines that start with <code>&lt;&lt;&lt;&lt;&lt;&lt;&lt;</code>, <code>&gt;&gt;&gt;&gt;&gt;&gt;&gt;</code>, or <code>=======</code>, mark the conflict as resolved and sync again with:</p> <pre><code>calkit overleaf sync --resolve\n</code></pre>"},{"location":"quickstart/","title":"Quickstart","text":""},{"location":"quickstart/#quickstart","title":"Quickstart","text":""},{"location":"quickstart/#from-an-existing-project","title":"From an existing project","text":"<p>If you want to use Calkit with an existing project, navigate into its working directory and use the <code>xr</code> command to start executing and recording your scripts, notebooks, LaTeX files, etc., as reproducible pipeline stages. For example:</p> <pre><code>calkit xr scripts/analyze.py\n\ncalkit xr notebooks/plot.ipynb\n\ncalkit xr paper/main.tex\n</code></pre> <p>Calkit will attempt to detect environments, inputs, and outputs and save them in <code>calkit.yaml</code>. If successful, you'll be able to run the full pipeline with:</p> <pre><code>calkit run\n</code></pre> <p>Next, make a change to e.g., a script and look at the output of <code>calkit status</code>. You'll see that the pipeline has a stage that is out-of-date:</p> <pre><code>---------------------------- Pipeline ----------------------------\nanalyze:\n        changed deps:\n                modified:           scripts/analyze.py\n</code></pre> <p>This can be fixed with another call to <code>calkit run</code>.</p> <p>You can save (add and commit) all changes with:</p> <pre><code>calkit save -am \"Add to pipeline\"\n</code></pre>"},{"location":"quickstart/#fresh-from-a-calkit-project-template","title":"Fresh from a Calkit project template","text":"<p>Create a new project from the <code>calkit/example-basic</code> template with:</p> <pre><code>calkit new project my-research \\\n    --title \"My research\" \\\n    --template calkit/example-basic \\\n    --cloud\n</code></pre> <p>Note the <code>--cloud</code> flag requires cloud integration to be set up, but can be omitted if the project doesn't need to be backed up to the cloud or shared with collaborators. Cloud integration can also be set up later.</p> <p>Next, move into the project folder and run the pipeline, which consists of several stages defined in <code>calkit.yaml</code>:</p> <pre><code>cd my-research\ncalkit run\n</code></pre> <p>Next, make some edits to a script or LaTeX file and run <code>calkit status</code> to see what stages are out-of-date. For example:</p> <pre><code>---------------------------- Pipeline ----------------------------\nbuild-paper:\n        changed deps:\n                modified:           paper/paper.tex\n</code></pre> <p>Execute <code>calkit run</code> again to bring everything up-to-date.</p> <p>To back up or save the project, call:</p> <pre><code>calkit save -am \"Run pipeline\"\n</code></pre>"},{"location":"references/","title":"References","text":""},{"location":"references/#references","title":"References","text":"<p>Each project can contain one or more reference collections. These are typically stored as <code>.bib</code> files, and listed in the <code>references</code> section of the <code>calkit.yaml</code> file.</p>"},{"location":"releases/","title":"Releasing/archiving projects and artifacts","text":""},{"location":"releases/#releasingarchiving-projects-and-artifacts","title":"Releasing/archiving projects and artifacts","text":"<p>When the project has reached an important milestone, e.g., a journal article is ready for submission, a release should be created to archive the relevant artifacts with a persistent identifier like a digital object identifier (DOI). The archived release should then be cited in the article so readers can follow the citation back to the project files in order to reproduce or reuse the results.</p>"},{"location":"releases/#integrating-with-zenodo","title":"Integrating with Zenodo","text":"<p>Calkit can archive whole projects or individual artifacts to Zenodo. To enable this functionality, you will either need to connect your Zenodo account with the Calkit Cloud or create a Zenodo personal access token (PAT) and set it in your machine's Calkit config or as an environmental variable.</p>"},{"location":"releases/#option-1-connecting-to-the-calkit-cloud","title":"Option 1: Connecting to the Calkit Cloud","text":"<p>Visit the Calkit Cloud user settings page and click the connect button to authorize the Calkit app to upload to Zenodo on your behalf.</p> <p></p> <p>The Calkit Cloud user settings page.</p>"},{"location":"releases/#option-2-using-a-zenodo-pat","title":"Option 2: Using a Zenodo PAT","text":"<p>If you don't already have a Zenodo PAT, first create one in your Zenodo account settings, then call:</p> <pre><code>calkit config set zenodo_token {paste Zenodo token here}\n</code></pre> <p>Alternatively, you may set your token as either the <code>ZENODO_TOKEN</code> or <code>CALKIT_ZENODO_TOKEN</code> environmental variable.</p>"},{"location":"releases/#creating-a-release-of-the-project","title":"Creating a release of the project","text":"<p>To create a new release of the entire project, execute:</p> <pre><code>calkit new release --name submitted-paper\n</code></pre> <p>The release name (<code>submitted-paper</code> above) should accurately and descriptively identify the release. For a research project, it might be better to use names of milestones rather than simple <code>v1</code>, <code>v2</code>, etc., You can also use the <code>--description</code> flag to add more details.</p> <p>When this is called, Calkit will:</p> <ul> <li>Compress and upload all files kept in Git and DVC to Zenodo,   which will produce a DOI,   ensuring the release can be accessed even if the repo is relocated.</li> <li>Create a Git tag. This can be used to create a release on GitHub if desired.</li> <li>Save the MD5 checksums of files kept in DVC in   <code>.calkit/releases/{release_name}/dvc-md5s.yaml</code>.   These can be used to populate the DVC cache from Zenodo later on.</li> <li>Create a <code>CITATION.cff</code> file to make the project easier to cite.</li> <li>Add a badge to the project's <code>README.md</code> file showing the release's DOI.</li> <li>Add the release to the <code>releases</code> section of the <code>calkit.yaml</code> file.</li> <li>Add a BibTeX entry for the release to a references file   (<code>references.bib</code> by default).</li> <li>Create a GitHub release with a link to the Zenodo record.</li> </ul>"},{"location":"releases/#releasing-other-types-of-artifacts-individually","title":"Releasing other types of artifacts individually","text":"<p>To release only one artifact, e.g., a dataset or publication, execute:</p> <pre><code>calkit new release \\\n    --name my-publication-v1 \\\n    --kind publication \\\n    path/to/the/publication.pdf\n</code></pre>"},{"location":"releases/#releasing-to-caltechdata","title":"Releasing to CaltechDATA","text":"<p>CaltechDATA is an instance of the InvenioRDM software that powers Zenodo, so archiving there is a similar process.</p> <p>First, set your CaltechDATA PAT with:</p> <pre><code>calkit config set caltechdata_token {paste CaltechDATA token here}\n</code></pre> <p>Then, in the <code>new release</code> command, simply add the <code>--to caltechdata</code> option.</p>"},{"location":"reproducibility/","title":"Reproducibility","text":""},{"location":"reproducibility/#reproducibility","title":"Reproducibility","text":"<pre><code>graph LR\n    A[Inputs] --&gt; B[Processes]\n    B --&gt; C[Outputs]\n</code></pre> <p>Reproducibility can be defined as:</p> <p>Given the inputs and process definitions, it's possible to generate the same outputs.</p> <p>Basically this means that if you have the code and data and run them, you get the same figures, tables, numerical results, etc.</p> <p>Inputs are not limited to code and data, however. They may include, but are not limited to:</p> <ul> <li>Raw data files</li> <li>Source code</li> <li>Configuration files</li> <li>Text files, e.g., for a manuscript</li> <li>Diagrams created with drawing tools like Inkscape</li> <li>Computational environment specifications</li> <li>Word documents</li> <li>CAD files</li> <li>Computational meshes, if created manually/interactively</li> </ul> <p>In other words, the inputs are primary artifacts. They were created from nothing or acquired from elsewhere, either by humans or AI agents.</p> <p>The main output of a research project is the article itself, which is typically in PDF form. This output is typically composed of the text input and some intermediate outputs or secondary artifacts like figures, tables, and numerical results.</p> <p>Process definitions describe how to turn the inputs into the outputs. For example:</p> <p>Run the analysis script with these arguments.</p> <p>Create the computational environment from the requirements.txt file.</p> <p>Compile the LaTeX source file to generate the PDF.</p>"},{"location":"reproducibility/#measuring-reproducibility","title":"Measuring reproducibility","text":"<p>A more measurable definition of reproducibility would be:</p> <p>The inverse of the time it takes to verify the outputs truly reflect the inputs and process definitions.</p> <p>One way to verify a project's reproducibility is to rerun all the processes and compare the outputs to those published. However, this can take a long time for projects with computationally expensive steps.</p> <p>Thankfully, this definition leaves open the possibility of not needing to rerun expensive processes by having some sort of traceability on the outputs, e.g., a <code>dvc.lock</code> file in a Calkit project. If we can trace through through the entire path, we can know the exact provenance.</p> <p>With this measurement methodology, a study that shares no code and data will be very hard to reproduce, if not impossible. One that shares data but no code will be a little better.</p> <p>Sharing code, environment lock files, and a fully-automated pipeline with file content-based tracking is the gold standard, however. Barring misconduct, provenance can be verified with a single command (<code>calkit status</code> for a Calkit project).</p>"},{"location":"reproducibility/#consequences-of-poor-reproducibility","title":"Consequences of poor reproducibility","text":"<p>On obvious consequence of poor reproducibility is that the research may not be trustworthy. The steps claimed to have been taken to produce evidence to back up a conclusion may not have actually been taken as described. This could simply be due to a lack of knowledge, memory, or they may be described imprecisely. Peer review is supposed to catch these sorts of instances in theory, but doing a reproducibility check is often a large undertaking.</p> <p>Furthermore, poor reproducibility indicates inefficient and error-prone project management practices. If multiple scripts/notebooks/commands need to be rerun to verify all outputs are up-to-date, it's more likely that they will be skipped, especially if they are expensive. The project may therefore end up in an inconsistent state, which may lead to publication of incorrect results. When iteration is slow and/or painful, fewer iterations will be done, and more iterations typically results in higher quality. It is therefore important to automate research projects, i.e., to keep them highly reproducible throughout their life cycles.</p>"},{"location":"version-control/","title":"Version control","text":""},{"location":"version-control/#version-control","title":"Version control","text":"<p>Version control is one of the pillars of reproducibility. However, the current de facto version control system (VCS), Git, was designed primarily for complex software development projects, and thus provides lots of control and flexibility, but with a daunting learning curve.</p> <p>Since most of the value of using version control for research projects comes from simply saving checkpoints of the project files so it's clear if they have changed, or so they can be reverted if something is broken, Calkit provides a simplified interface that focuses on just that. Additionally, since Git was not designed for large and/or binary files, Calkit uses DVC to version these file types.</p> <p>GitHub is currently the most popular location to back up Git repositories, or repos, in the cloud, but like Git, is primarily designed for software development. Similar to how Calkit is a layer on top of Git, The Calkit Cloud (calkit.io) integrates with GitHub to add a more purpose-built interface for research projects. It also serves as a default DVC remote, so users are not required to provision their own.</p> <p>Though Calkit adds a simplified interface on top of Git and DVC, the lower-level tools <code>git</code> and <code>dvc</code> can be used if desired, e.g., for more complex operations.</p>"},{"location":"version-control/#typical-workflow","title":"Typical workflow","text":"<p>In order to start working on a project, the project repository must exist on your local machine. This can be achieved either by creating a new repo or downloading, or \"cloning,\" an existing one from the cloud. After a repo exists on your local machine, it is typical to repeat the cycle of committing new or changed files with a message describing them, and then pushing those commits to the cloud. This can be achieved with three workflow variants that trade off automation for control.</p> <p>The simplest and most hands-off uses <code>calkit save</code>, which will automatically make decisions about which files belong in Git which belong in DVC, which don't belong in either, commit them, and push them to the cloud all with a single command:</p> <pre><code>graph LR\n    init[calkit clone\n         or\n         calkit new project]\n    --&gt; edit[create or\n             edit files]\n    --&gt; save[calkit save -am\n             'Your message here']\n    --&gt; edit\n</code></pre> <p>The <code>-a</code> flag indicates that we want to save all relevant files and the <code>-m</code> flag indicates that we are providing a message describing the changes. If omitted, Calkit will prompt the user for a message.</p> <p>If more control is desired, the <code>save</code> step can be broken down into <code>add</code>, <code>commit</code> and <code>push</code> steps.</p> <pre><code>graph LR\n    init[calkit clone\n         or\n         calkit new project]\n    --&gt; edit[create or\n             edit files]\n    --&gt; add[\"calkit add\n            {path1}\n            {path2}\n            ...\"]\n    --&gt; commit[calkit commit -m\n               'Your message here']\n    --&gt; push[calkit push]\n    --&gt; edit\n</code></pre> <p>The <code>add</code> step can be skipped for files that have been previously committed by adding the <code>-a</code> flag:</p> <pre><code>graph LR\n    init[calkit clone\n         or\n         calkit new project]\n    --&gt; edit[create or\n             edit files]\n    --&gt; commit[calkit commit -am\n               'Your message here']\n    --&gt; push[calkit push]\n    --&gt; edit\n</code></pre> <p>If you have a collaborator working on the same project, any time they have pushed commits, you will need to pull before being able to push yourself.</p>"},{"location":"version-control/#command-reference","title":"Command reference","text":"<p>To view the help for any of these commands, execute <code>calkit {command} --help</code>.</p>"},{"location":"version-control/#clone","title":"<code>clone</code>","text":"<p><code>calkit clone</code> will download and create a local copy of the project, setup the default Calkit DVC remote and pull any files versioned with DVC. The multi-step equivalent would be:</p> <ul> <li><code>git clone</code></li> <li><code>calkit config remote</code></li> <li><code>dvc pull</code></li> </ul> <p>If the project is hosted on the Calkit Cloud, it can be referenced by name rather than Git repo URL. For example:</p> <pre><code>calkit clone petebachant/strava-analysis\n</code></pre>"},{"location":"version-control/#status","title":"<code>status</code>","text":"<p><code>calkit status</code> will show the combined status from both Git and DVC. For example:</p> <pre><code>$ calkit status\n--------------------------- Code (Git) ---------------------------\nOn branch main\nnothing to commit, working tree clean\n\n--------------------------- Data (DVC) ---------------------------\nNo changes.\n\n------------------------- Pipeline (DVC) -------------------------\nData and pipelines are up to date.\n</code></pre>"},{"location":"version-control/#save","title":"<code>save</code>","text":"<p><code>calkit save</code> will create a commit and push to the remotes in one step. It will automatically any ignore any files it deems to be inappropriate to save in version control. This provides the most automated and hands of experience but gives the least control.</p> <p>Options:</p> <ul> <li><code>[PATHS]...</code>: Specify a list of paths to save. Not required if <code>--all</code> is   specified.</li> <li><code>--all</code>, <code>-a</code>: Save all paths.</li> <li><code>--to</code>, <code>-t</code>: Manually specify <code>git</code> or <code>dvc</code> as the tracking mechanism.</li> <li><code>--message</code>, <code>-m</code>: Specify a commit message. If omitted, the user will be   prompted for one.</li> <li><code>--no-push</code>: Do not push after committing.</li> </ul>"},{"location":"version-control/#sync","title":"<code>sync</code>","text":"<p><code>calkit sync</code> will pull from then push to the cloud, ensuring both copies are in sync.</p>"},{"location":"version-control/#add","title":"<code>add</code>","text":"<p><code>calkit add</code> will add a file to the repo \"staging area,\" which sets it up to be committed. Calkit will determine based on its type and size if it should be tracked with Git or DVC and act accordingly.</p> <p>Options:</p> <ul> <li><code>--to</code>, <code>-t</code>: Manually specify <code>git</code> or <code>dvc</code> as the tracking mechanism.</li> <li><code>--commit-message</code>, <code>-m</code>: Create a commit after adding   and use the provided message.</li> <li><code>--auto-commit-message</code>, <code>-M</code>: Commit with an automatically-generated message.   Only compatible when adding one path.</li> <li><code>--push</code>: Push to the Git or DVC remote after committing.</li> </ul>"},{"location":"pipeline/","title":"The pipeline","text":""},{"location":"pipeline/#the-pipeline","title":"The pipeline","text":"<p>The pipeline defines and ties together the processes that produce the project's important assets or artifacts, such as datasets, figures, tables, and publications. It is saved in the <code>pipeline</code> section of the <code>calkit.yaml</code> file, and is compiled to a DVC pipeline (saved in <code>dvc.yaml</code>) when <code>calkit run</code> is called.</p> <p>A pipeline is composed of stages, each of which has a specific type or \"kind.\" Each stage must specify the environment in which it runs to ensure it's reproducible. Calkit will automatically generate an \"environment lock file\" at the start of a run and can therefore automatically detect if an environment has changed, and the affected stages need to be rerun. Stages can also define <code>inputs</code> and <code>outputs</code>, and you can decide how you'd like outputs to be stored, i.e., with Git or DVC.</p> <p>Any stages that have not changed since they were last run will be skipped, since their results will have been cached.</p> <p>In the <code>calkit.yaml</code> file, you can define a <code>pipeline</code> (and <code>environments</code>) like:</p> <pre><code># Define environments\nenvironments:\n  main:\n    kind: uv-venv\n    path: requirements.txt\n    python: \"3.13\"\n  texlive:\n    kind: docker\n    image: texlive/texlive:latest-full\n\n# Define the pipeline\npipeline:\n  stages:\n    collect-data:\n      kind: python-script\n      script_path: scripts/collect-data.py\n      environment: main\n      outputs:\n        - data/raw.csv\n        - path: data/meta.json\n          storage: git\n          delete_before_run: false\n    process-data:\n      kind: jupyter-notebook\n      notebook_path: notebooks/process.ipynb\n      environment: main\n      inputs:\n        - data/raw.csv\n      outputs:\n        - data/processed.csv\n        - figures/fig1.png\n    build-paper:\n      kind: latex\n      target_path: paper/paper.tex\n      environment: texlive\n      inputs:\n        - figures/fig1.png\n        - references.bib\n</code></pre>"},{"location":"pipeline/#stage-types-and-unique-attributes","title":"Stage types and unique attributes","text":"<p>All stage declarations require a <code>kind</code> and an <code>environment</code>, and can specify <code>inputs</code> and <code>outputs</code>. The different kinds of stages and their unique attributes are listed below. For more details, see <code>calkit.models.pipeline</code>.</p>"},{"location":"pipeline/#python-script","title":"<code>python-script</code>","text":"<ul> <li><code>script_path</code></li> <li><code>args</code> (list, optional)</li> </ul>"},{"location":"pipeline/#shell-command","title":"<code>shell-command</code>","text":"<ul> <li><code>command</code></li> <li><code>shell</code> (optional, e.g., <code>bash</code>, <code>sh</code>, <code>zsh</code>; default: <code>bash</code>)</li> </ul>"},{"location":"pipeline/#shell-script","title":"<code>shell-script</code>","text":"<ul> <li><code>script_path</code></li> <li><code>shell</code> (optional, e.g., <code>bash</code>, <code>sh</code>, <code>zsh</code>; default: <code>bash</code>)</li> <li><code>args</code> (list, optional)</li> </ul>"},{"location":"pipeline/#matlab-script","title":"<code>matlab-script</code>","text":"<ul> <li><code>script_path</code></li> </ul>"},{"location":"pipeline/#latex","title":"<code>latex</code>","text":"<ul> <li><code>target_path</code></li> </ul>"},{"location":"pipeline/#docker-command","title":"<code>docker-command</code>","text":"<ul> <li><code>command</code></li> </ul>"},{"location":"pipeline/#r-script","title":"<code>r-script</code>","text":"<ul> <li><code>script_path</code></li> <li><code>args</code> (list, optional)</li> </ul>"},{"location":"pipeline/#julia-script","title":"<code>julia-script</code>","text":"<ul> <li><code>script_path</code></li> <li><code>args</code></li> </ul>"},{"location":"pipeline/#julia-command","title":"<code>julia-command</code>","text":"<ul> <li><code>command</code></li> </ul>"},{"location":"pipeline/#sbatch","title":"<code>sbatch</code>","text":"<ul> <li><code>script_path</code></li> <li><code>args</code></li> <li><code>sbatch_options</code></li> </ul> <p>This stage type runs a script with <code>sbatch</code>, which is a common way to run jobs on a high performance computing (HPC) cluster that uses the SLURM job scheduler.</p>"},{"location":"pipeline/#iteration","title":"Iteration","text":""},{"location":"pipeline/#over-a-list-of-values","title":"Over a list of values","text":"<pre><code>pipeline:\n  stages:\n    my-iter-stage:\n      kind: python-script\n      script_path: scripts/my-script.py\n      args:\n        - \"--model={var}\"\n      iterate_over:\n        - arg_name: var\n          values:\n            - some-model\n            - some-other-model\n      inputs:\n        - data/raw\n      outputs:\n        - models/{var}.h5\n</code></pre>"},{"location":"pipeline/#over-a-table-or-list-of-lists","title":"Over a table (or list of lists)","text":"<pre><code>pipeline:\n  stages:\n    my-iter-stage:\n      kind: python-script\n      script_path: scripts/my-script.py\n      args:\n        - \"--model={var1}\"\n        - \"--n_estimators={var2}\"\n      iterate_over:\n        - arg_name: [var1, var2]\n          values:\n            - [some-model, 5]\n            - [some-other-model, 7]\n      inputs:\n        - data/raw\n      outputs:\n        - models/{var1}-{var2}.h5\n</code></pre>"},{"location":"pipeline/#over-ranges-of-numbers","title":"Over ranges of numbers","text":"<pre><code>pipeline:\n  stages:\n    my-iter-stage:\n      kind: python-script\n      script_path: scripts/my-script.py\n      args:\n        - \"--thresh={thresh}\"\n      iterate_over:\n        - arg_name: thresh\n          values:\n            - range:\n                start: 0\n                stop: 20\n                step: 0.5\n            - range:\n                start: 30\n                stop: 35\n                step: 1\n            - 41\n      inputs:\n        - data/raw\n      outputs:\n        - results/{thresh}.csv\n</code></pre>"},{"location":"pipeline/#automatic-stage-and-environment-detection","title":"Automatic stage and environment detection","text":"<p>The <code>calkit xr</code> command, which stands for \"execute and record,\" can be used to automatically generate pipeline stages and environments from scripts (Python, MATLAB, Julia, R, and shell), notebooks, LaTeX source files, or shell commands.</p> <p>For example, if you have a Python script in <code>scripts/run.py</code>, you can call:</p> <pre><code>calkit xr scripts/run.py\n</code></pre> <p>Calkit will attempt to detect which environment in which this script should run, creating one if necessary (it can also be specified with the <code>-e</code> flag.) Calkit will then try to detect inputs and outputs and attempt to run the stage it created. If successful, it will be added to the pipeline and kept reproducible from that point onwards. That is, calling <code>calkit run</code> again will detect if the script, environment, or any input files have changed, and rerun if so.</p>"},{"location":"pipeline/manual-steps/","title":"Manual steps","text":""},{"location":"pipeline/manual-steps/#manual-steps","title":"Manual steps","text":"<p>Sometimes there are steps in a pipeline that are too cumbersome to automate, but are not complex enough to warrant defining an entire procedure. Or maybe they will not need to be iterated much, so automation would not be worth the trouble.</p> <p>For example, imagine you want to save a mesh snapshot image from a CFD simulation using ParaView. You can use the <code>calkit manual-step</code> command to pause the pipeline, execute a command to open ParaView, and display a message with instructions.</p> <pre><code># In dvc.yaml\nstages:\n  save-mesh-snapshot-isometric:\n    cmd: &gt;\n      calkit manual-step\n      --cmd\n      \"touch sim/cases/k-epsilon-ny-40/case.foam &amp;&amp; paraview sim/cases/k-epsilon-ny-40/case.foam\"\n      --message\n      \"Save isometric mesh image to figures/rans-mesh-snapshot-isometric.png\"\n    deps:\n      - sim/cases/k-epsilon-ny-40/constant/polyMesh\n    outs:\n      - figures/rans-mesh-snapshot-isometric.png\n</code></pre> <p>After confirming, DVC will check that the defined outputs exist, and if so, continue with the pipeline execution.</p>"},{"location":"pipeline/running-and-logging/","title":"Running and logging","text":""},{"location":"pipeline/running-and-logging/#running-and-logging","title":"Running and logging","text":"<p>When the pipeline is run with the <code>calkit run</code> command, Calkit will first compile the pipeline into a DVC pipeline with some additional stages to handle environment checking.</p> <p>If the <code>--log</code> option is specified, Calkit also collects important system information such as foundational dependency versions to log for the sake of traceability and saves to JSON files in <code>.calkit/systems</code>.</p> <p>While the run is executing, DVC logs will be sent into <code>.calkit/logs</code> and after it's complete, run metadata will be saved to a JSON file in <code>.calkit/runs</code>. Again, these can be helpful for traceability and diagnosing reproducibility issues down the road, e.g., if the project is being run on multiple machines and the results are different between them.</p> <p>The run metadata can be queried and analyzed, for example, with DuckDB:</p> <pre><code>import duckdb\n\nduckdb.sql(\n    \"\"\"\n        select\n            system.node_id,\n            system.calkit_version,\n            cast(end_time as timestamp)\n                - cast(start_time as timestamp) duration,\n            status\n            from '.calkit/runs/*.json' run\n            left join '.calkit/systems/*.json' system\n                on run.system_id = system.id\n            where run.dvc_args = '[]'\n    \"\"\"\n)\n</code></pre> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502     node_id     \u2502 calkit_version \u2502    duration     \u2502 status  \u2502\n\u2502      int64      \u2502    varchar     \u2502    interval     \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 138587250590302 \u2502 0.26.0         \u2502 00:00:02.481729 \u2502 success \u2502\n\u2502 138587250590302 \u2502 0.26.0         \u2502 00:00:02.57566  \u2502 success \u2502\n\u2502 138587250590302 \u2502 0.26.0         \u2502 00:00:04.728486 \u2502 success \u2502\n\u2502 138587250590302 \u2502 0.26.0         \u2502 00:00:08.676203 \u2502 success \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"pipeline/slurm/","title":"SLURM integration","text":""},{"location":"pipeline/slurm/#slurm-integration","title":"SLURM integration","text":"<p>Calkit can run pipeline stages on a SLURM job scheduler using the <code>slurm</code> environment and <code>sbatch</code> stage types. The <code>calkit slurm</code> CLI can then be used to monitor these jobs by their name in the context of a project.</p> <p>For example, let's create a <code>calkit.yaml</code> file with a <code>slurm</code> environment and two <code>sbatch</code> stages:</p> <pre><code># In calkit.yaml\nenvironments:\n  my-cluster:\n    kind: slurm\n    host: my.cluster.somewhere.edu\n\npipeline:\n  stages:\n    sim:\n      kind: sbatch\n      environment: my-cluster\n      script_path: scripts/run-sim.sh\n      inputs:\n        - config/my-sim-config.yaml\n      outputs:\n        - results/all.h5\n      sbatch_options:\n        - --time=60\n    post-process:\n      kind: sbatch\n      environment: my-cluster\n      script_path: scripts/post.sh\n      inputs:\n        - results/all.h5\n      outputs:\n        - results/post.h5\n        - figures/myfig.png\n      sbatch_options:\n        - --gpus=1\n        - --time=20\n</code></pre> <p>When calling <code>calkit run</code>, as long as we're running from the project directory on the host <code>my.cluster.somewhere.edu</code>, the <code>run-sim</code> job will be submitted. By default, Calkit will wait for the job to finish, but will be robust to disconnecting. That is, if you disconnect and reconnect (or simply exit with <code>ctrl+c</code>), calling <code>calkit run</code> will check if the job is still running and wait for it if so.</p> <p>If we wanted to submit both jobs at the same time, we could call <code>calkit run sim</code>, press <code>ctrl+c</code> to stop waiting, then call <code>calkit run post-process</code>.</p> <p>If we want to check the status of any of the project's jobs, we can call <code>calkit slurm queue</code>, and if we wanted to cancel one, we can cancel it by name, e.g., <code>calkit slurm cancel post-process</code>.</p>"},{"location":"tutorials/","title":"Tutorials","text":""},{"location":"tutorials/#tutorials","title":"Tutorials","text":"<ul> <li>Creating your first project</li> <li>Composing a pipeline from multiple Jupyter notebooks in JupyterLab</li> <li>Converting an existing project to a Calkit project</li> <li>Reproducible OpenFOAM simulations</li> <li>LaTeX collaboration with GitHub Codespaces</li> <li>Jupyter notebook as a DVC pipeline</li> <li>Keeping track of conda environments</li> <li>Defining and executing manual procedures</li> <li>Adding a new LaTeX-based publication with its own Docker build environment</li> <li>A reproducible workflow using Microsoft Office (Word and Excel)</li> <li>Using Calkit with MATLAB</li> <li>Automation with GitHub Actions</li> </ul>"},{"location":"tutorials/adding-latex-pub-docker/","title":"Adding a new LaTeX-based publication with its own Docker build environment","text":""},{"location":"tutorials/adding-latex-pub-docker/#adding-a-new-latex-based-publication-with-its-own-docker-build-environment","title":"Adding a new LaTeX-based publication with its own Docker build environment","text":"<p>Have you ever wanted to collaborate with a team on a LaTeX article, but have ran into roadblocks getting everyone on the team to install the correct dependencies? This can be especially difficult if different team members are using different operating systems. However, this is a perfect use case for building the paper with a Docker container. Here's how to do that by creating a publication in your project and specifying a LaTeX template and Docker environment in which to build it, all with one command:</p> <pre><code>calkit new publication \\\n    --title \"This is the title\" \\\n    --description \"This is the description of the paper.\" \\\n    --kind journal-article \\\n    --template latex/article \\\n    --environment latex \\\n    --stage build-paper \\\n    ./paper\n</code></pre> <p>What happens when we do this:</p> <ol> <li>A new publication is added to <code>calkit.yaml</code>.</li> <li>A new Docker environment called <code>latex</code> is added to <code>calkit.yaml</code>.    This environment uses an <code>_include</code> key so the details can be written to    a different file, <code>.calkit/environments/latex.yaml</code>.    This will allow us to use that environment specification as an input    dependency for a DVC pipeline stage,    such that if our environment changes, that stage will be rerun.</li> <li>Files from a LaTeX template called \"article\" are copied into the <code>./paper</code>    directory.</li> <li>A new stage called <code>build-paper</code> is added to the DVC pipeline in <code>dvc.yaml</code>.    It will have dependencies based on source files in <code>./paper</code>    and an output based on the template's target file.    Note that you can add more dependencies to the resulting pipeline stage    with the <code>--dep</code> and <code>--deps-from-stage-outs</code> commands.</li> <li>A Git commit is made automatically to add all of these files to the repo.    Note this can be disabled with the <code>--no-commit</code> option.</li> </ol> <p>If you need to add more dependencies to the stage later, e.g., if you have a <code>.bib</code> file for references, or add more figures, you can add these by editing <code>dvc.yaml</code> directly.</p>"},{"location":"tutorials/conda-envs/","title":"Keeping track of conda environments","text":""},{"location":"tutorials/conda-envs/#keeping-track-of-conda-environments","title":"Keeping track of conda environments","text":"<p>It can be difficult to know if a conda environment present on your machine matches one in your project's <code>environment.yml</code> file. You may be collaborating with a team on a project and someone adds a dependency, then all of a sudden things won't run on your machine. Or maybe you use multiple machines to run the same project.</p> <p>Calkit has a feature to make working with conda environments more reproducible, without needing to rebuild the environment all the time. If you're working on a project with a conda <code>environment.yml</code> file, you can add it to the project's environments in <code>calkit.yaml</code>:</p> <pre><code>environments:\n  my-conda:\n    kind: conda\n    path: environment.yml\n</code></pre> <p>Then, any time a command is run in that environment, e.g., with:</p> <pre><code>calkit xenv -n my-conda -- python -c \"print('hello world')\"\n</code></pre> <p>the environment on your local machine will be rebuilt if it doesn't match the spec, or it will be created if it doesn't exist. Note that this will delete the existing environment and rebuild from scratch, so make sure you don't have any unsaved changes in there. Also note that for some combinations of <code>pip</code> dependencies, it may not be possible to arrive at an environment that matches the spec, so it is recommended to only put the \"top-level\" dependencies in <code>environment.yml</code> rather than a full export.</p>"},{"location":"tutorials/conda-envs/#adding-a-conda-environment-to-a-calkit-project","title":"Adding a Conda environment to a Calkit project","text":"<p>If you run something like:</p> <pre><code>calkit new conda-env \\\n    -n my-project-py311 \\\n    python=3.11 \\\n    pip \\\n    matplotlib \\\n    pandas \\\n    jupyter \\\n    --pip tensorflow\n</code></pre> <p>Calkit will create an environment definition in <code>calkit.yaml</code> and a corresponding <code>environment.yml</code> file. If you need multiple conda environments, you can run this command multiple times, changing the <code>--path</code> option.</p>"},{"location":"tutorials/existing-project/","title":"Converting an existing project to a Calkit project","text":""},{"location":"tutorials/existing-project/#converting-an-existing-project-to-a-calkit-project","title":"Converting an existing project to a Calkit project","text":"<p>Note</p> <p>This tutorial requires Calkit version 0.19.0 or above. If the output of <code>calkit --version</code> shows a lower version, run <code>pip install --upgrade calkit</code> or <code>calkit upgrade</code> to upgrade.</p> <p>In this tutorial we're going to convert an existing project into a Calkit project, assuming we've never done anything like this before. Thus, the project is not yet using any version control or pipeline management system. We're also going to do everything in the most automated and hands-off way possible. More flexibility can be achieved with the lower-level interfaces, but for now, we just want to make the project reproducible as quickly as possible with reasonable defaults.</p> <p>Before we get started, make sure that Calkit is installed, you have an account on calkit.io, and have set a token in your local config.</p> <p>The basic steps we'll take here are:</p> <ol> <li>Organize the project folder.</li> <li>Create a new Calkit project.</li> <li>Add all existing files to version control and back them up in the cloud.</li> <li>Add all computational processes to the pipeline, ensuring they run in    defined environments.</li> <li>Define the project artifacts for presentation and consumption.</li> </ol>"},{"location":"tutorials/existing-project/#organize-the-project-folder","title":"Organize the project folder","text":"<p>The first step is to collect all of the files relevant to the project and ensure they are in a single parent folder. If you're a grad student, you might work on a single topic throughout grad school, which means all of your research-related files can go into a single project. Note that we don't want to include things like coursework or personal documents like your CV or transcripts. The folder should only include materials relevant to planning, performing, and publishing the research. Anything to be shared with the outside world, and anything required to produce those things should be included. If you have a script referencing some data outside this parent folder, move the data inside and update the script accordingly.</p> <p>Here's an example project folder layout:</p> <pre><code>\ud83d\udcc2 my-phd-research\n\u251c\u2500\u2500 \ud83d\udcc2 data\n\u2502   \u251c\u2500\u2500 \ud83d\udcc2 raw\n\u2502   \u2514\u2500\u2500 \ud83d\udcc2 processed\n\u251c\u2500\u2500 \ud83d\udcc2 docs\n\u2502   \u2514\u2500\u2500 \ud83d\udcdc notes.md\n\u251c\u2500\u2500 \ud83d\udcc2 figures\n\u2502   \u251c\u2500\u2500 \ud83d\udcdc plot1.png\n\u2502   \u2514\u2500\u2500 \ud83d\udcdc plot2.png\n\u251c\u2500\u2500 \ud83d\udcc2 pubs\n\u2502   \u251c\u2500\u2500 \ud83d\udcc2 proposal\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcdc proposal.pdf\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcdc proposal.tex\n\u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcdc README.md\n\u2502   \u251c\u2500\u2500 \ud83d\udcc2 2025-article-1\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcdc paper.pdf\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcdc paper.tex\n\u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcdc README.md\n\u2502   \u251c\u2500\u2500 \ud83d\udcc2 2025-aps-dfd-slides\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcdc slides.pdf\n\u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcdc slides.tex\n\u2502   \u251c\u2500\u2500 \ud83d\udcc2 thesis\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc2 chapters\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcdc README.md\n\u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcdc slides.pptx\n\u251c\u2500\u2500 \ud83d\udcc2 scripts\n\u2502   \u251c\u2500\u2500 \ud83d\udcdc plot.py\n\u2502   \u2514\u2500\u2500 \ud83d\udcdc process.py\n\u251c\u2500\u2500 \ud83d\udcc2 simulations\n\u2502   \u251c\u2500\u2500 \ud83d\udcc2 case1\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcdc config.txt\n\u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcdc output.h5\n\u2502   \u251c\u2500\u2500 \ud83d\udcc2 case2\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcdc config.txt\n\u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcdc output.h5\n\u2502   \u2514\u2500\u2500 \ud83d\udcdc run.py\n\u2514\u2500\u2500 \ud83d\udcdc references.bib\n</code></pre> <p>It's okay if the structure doesn't match exactly. It's just important that everything is in there. You can reorganize later. We're mainly focused on minimizing external dependencies to improve reproducibility. That is, the more self-contained we can make the project, the easier it will be to reproduce, since getting all of those external dependencies documented properly and setup in a different context can be a challenge.</p> <p>It's a good idea to keep your library of references (the BibTeX file <code>references.bib</code> in the example above) in the project folder, rather than having any of your publications reference a file outside the project, e.g., if you have a \"global\" BibTeX file, or a reference collection in an app like Zotero.</p> <p>Similarly, if you have files in cloud services like Dropbox or Overleaf, download all of them to the project folder. This project folder should be the single source of truth. You can work on materials in other tools, but if so, the files should always be downloaded back to the main project folder.</p> <p>Tip</p> <p>Don't be afraid to repeat yourself in code. There is a software engineering principle \"don't repeat yourself,\" (DRY), which if applied too aggressively, can make it very difficult to track dependencies, which is crucial to maintaining reproducibility and simplicity.</p> <p>For example, imagine the <code>plot.py</code> and <code>process.py</code> script both contain similar logic for reading in raw data. One might be tempted to put this logic into a separate module so it's not written twice, but someday the requirement for plotting may change slightly, and if this module is a dependency for processing, technically the processing should be rerun to ensure reproducibility. If the processing is expensive, this could be wasteful.</p> <p>It's a good rule of thumb to wait until you've repeated a block of code three times before \"abstracting\" that logic into its own separate piece of code. That way, you can see how it's used and use the interface that emerged rather than attempting to design one from the start.</p>"},{"location":"tutorials/existing-project/#createinitialize-the-project","title":"Create/initialize the project","text":"<p>With a terminal open inside the project folder (<code>my-phd-research</code> in the example above), initialize it as a new Calkit project with:</p> <pre><code>calkit new project . \\\n    --name my-phd-research \\\n    --title \"Experimental investigation of something\" \\\n    --description \"Investigating the effects of a thing.\" \\\n    --cloud\n</code></pre> <p>In this command, the <code>.</code> means the current working directory, or \"here.\" The name, title, and description should be adapted to your own project of course. The name should be \"kebab-case\" (all lowercase with hyphens separating words), the title should be sentence or title case, and the description should include punctuation, kind of like an abstract.</p> <p>The <code>--cloud</code> flag is going to create a GitHub repo and Calkit Cloud project for us, which will be linked together. In the next step, when we put the files in version control, the code and text files will go to GitHub, and the larger data files will go to the Calkit Cloud. This will be handled seamlessly and transparently.</p> <p>Note you can add a <code>--public</code> flag if you want the project to be public from the get go. This is encouraged but can be a little worrying at first. The project can always be made public later, so let's start with it private for now.</p> <p>To summarize, this command will:</p> <ul> <li>Initialize a Git repository with GitHub as the remote</li> <li>Initialize a DVC configuration with the Calkit Cloud as the remote</li> <li>Create a <code>calkit.yaml</code> file for the project metadata</li> <li>Create a dev container specification in <code>.devcontainer</code> for use with VS Code   or GitHub Codespaces</li> <li>Create a basic <code>README.md</code> file</li> </ul>"},{"location":"tutorials/existing-project/#put-everything-in-version-control","title":"Put everything in version control","text":"<p>Now that we have everything in one project folder and we have the project created in the cloud, it's time to add files to version control. If you run <code>calkit status</code>, you'll see an output like:</p> <pre><code>$ calkit status\n---------------------------- Project -----------------------------\nProject status not set. Use \"calkit new status\" to update.\n\n--------------------------- Code (Git) ---------------------------\nOn branch main\nYour branch is up to date with 'origin/main'.\n\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n        .DS_Store\n        data/\n        docs/\n        figures/\n        pubs/\n        references.bib\n        scripts/\n        simulations/\n\nnothing added to commit but untracked files present (use \"git add\" to track)\n\n--------------------------- Data (DVC) ---------------------------\nNo changes.\n\n------------------------- Pipeline (DVC) -------------------------\nThere are no data or pipelines tracked in this project yet.\nSee &lt;https://dvc.org/doc/start&gt; to get started!\n</code></pre> <p>We have a list of files and folders that are untracked, meaning they are not in version control yet. We could add these with either <code>git add</code> or <code>dvc add</code>, or we can let Calkit decide which makes the most sense depending on the file type and size.</p> <p>If you're a Mac user, you'll notice the <code>.DS_Store</code> file, which is not something we want to keep in version control. We can ignore that file with <code>calkit ignore .DS_Store</code>. When you run <code>calkit status</code> again, you'll notice that file is no longer in the list of untracked files, which is exactly what we want. You can use <code>calkit ignore</code> with any other files or folders you want to keep out of version control, but keep in mind that when something is not in version control, it's not available to collaborators, and won't be present in another copy of the project repo elsewhere, e.g., on a different computer.</p> <p>Now let's go through our untracked folders one by one and start adding them to the repo. We can start with by running <code>calkit add</code> on <code>data/raw</code>:</p> <pre><code>$ calkit add data/raw -M\nAdding data/raw to DVC since it's greater than 1 MB\n100% Adding...|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|1/1 [00:00, 58.23file/s]\n[main ee7b35b] Add data/raw\n 2 files changed, 7 insertions(+)\n create mode 100644 data/.gitignore\n create mode 100644 data/raw.dvc\n</code></pre> <p>In the output, Calkit explains why that folder was added to DVC. Note that we also used the <code>-M</code> flag, which will automatically generate a commit message for us. If you'd like to specify your own message, use <code>-m</code> instead. You can see a list of all commits with <code>git log</code>.</p> <p>Repeat the <code>calkit status</code> and <code>calkit add</code> process with each of the files and folders until there are no more untracked files. Be careful adding folders with lots of other files and folders inside. It's usually a good idea to add these more granularly instead of all at once. The <code>pubs</code> directory in our example is one such case. There are PDFs in there, which typically belong in DVC instead of Git, and there may be LaTeX output logs and intermediate files, which should typically be ignored.</p> <pre><code>$ calkit add pubs/2025-aps-dfd-slides/slides.tex -M\n\nAdding pubs/2025-aps-dfd-slides/slides.tex to Git\n[main d680687] Add pubs/2025-aps-dfd-slides/slides.tex\n 1 file changed, 0 insertions(+), 0 deletions(-)\n create mode 100644 pubs/2025-aps-dfd-slides/slides.tex\n</code></pre> <pre><code>$ calkit add pubs/2025-aps-dfd-slides/slides.pdf -M\n\nAdding pubs/2025-aps-dfd-slides/slides.pdf to DVC per its extension\n100% Adding...|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|1/1 [00:00, 99.20file/s]\n[main 757042b] Add pubs/2025-aps-dfd-slides/slides.pdf\n 2 files changed, 6 insertions(+)\n create mode 100644 pubs/2025-aps-dfd-slides/.gitignore\n create mode 100644 pubs/2025-aps-dfd-slides/slides.pdf.dvc\n</code></pre> <p>If you want to manually control whether a target is tracked with Git or DVC, you can use the <code>--to=git</code> or <code>--to=dvc</code> option. Also, if you make a mistake along the way you can use the <code>git revert</code> command, after finding the offending commit with <code>git log</code>.</p>"},{"location":"tutorials/existing-project/#back-up-the-project-in-the-cloud","title":"Back up the project in the cloud","text":"<p>After all relevant files are added and committed to the repo, we can push to both GitHub and the Calkit Cloud with <code>calkit push</code>:</p> <pre><code>$ calkit push\nPushing to Git remote\nEnumerating objects: 41, done.\nCounting objects: 100% (41/41), done.\nDelta compression using up to 10 threads\nCompressing objects: 100% (29/29), done.\nWriting objects: 100% (37/37), 3.29 KiB | 3.29 MiB/s, done.\nTotal 37 (delta 11), reused 0 (delta 0), pack-reused 0\nremote: Resolving deltas: 100% (11/11), completed with 1 local object.\nTo https://github.com/your-name/my-phd-research\n   dc09efe..8f21641  main -&gt; main\nPushing to DVC remote\nChecking authentication for DVC remote: calkit\nCollecting                                                    |57.0 [00:00, 2.73kentry/s]\nPushing\n42 files pushed\n</code></pre>"},{"location":"tutorials/existing-project/#add-all-computational-processes-to-the-pipeline","title":"Add all computational processes to the pipeline","text":"<p>Now that we have all of our files in version control, we need to ensure that our output artifacts like derived datasets, figures, and publication PDFs are generated with the pipeline. This will ensure that they stay up to date if any of their input data or dependencies change.</p> <p>But first, before building the pipeline, we need to define computational environments to use in the stages. This is important for reproducibility since our results will be less dependent on the unique state of our local machine. Others looking to reproduce our work will only need to have the environment management software installed, and the specific applications or packages needed will be installed and used automatically by Calkit.</p>"},{"location":"tutorials/existing-project/#create-computational-environments","title":"Create computational environments","text":"<p>This project uses Python scripts, so we'll first want to define an environment in which these will run. If we want to use Conda (and it's installed), we can call:</p> <pre><code>calkit new conda-env --name py pandas matplotlib\n</code></pre> <p>In the command above we're specifying two packages to exist in the environment, <code>pandas</code> and <code>matplotlib</code>. However, you may have many more than this. You can add them to the command or add them to the resulting environment definition file (<code>environment.yml</code> by default for Conda environments) later. If you prefer Python's built-in <code>venv</code> module to manage your environment, you can replace <code>conda-env</code> with <code>venv</code>, and similarly, if you prefer <code>uv</code>, you can replace it with <code>uv-venv</code>.</p> <p>After an environment is created, it will be stored in the <code>environments</code> section of the <code>calkit.yaml</code> file. It can also be modified (or removed) by editing that file.</p> <p>If you have multiple Python scripts that require different, possibly conflicting sets of packages, you can simply create multiple environments and name them descriptively. For example, instead of one environment called <code>py</code>, you can create one called <code>processing</code> and one called <code>plotting</code>.</p> <p>If you aren't using Python, you can create other types of environments. The main goal is to ensure that all processes are run in one if possible. See the environments documentation for more information.</p> <p>The project also compiles some LaTeX documents. We can create a Docker environment called <code>tex</code> for these with:</p> <pre><code>calkit new docker-env --name tex --image texlive/texlive:latest-full\n</code></pre> <p>This environment is referencing a TeXLive Docker image from Docker Hub, which requires Docker to be installed, but will not require a separate LaTeX distribution to be installed. If you don't need the full TeXLive distribution, you can select any other image you'd like from this list.</p>"},{"location":"tutorials/existing-project/#add-pipeline-stages","title":"Add pipeline stages","text":"<p>Now we can create a stage for all of our important outputs. For each of these, we'll define what kind of stage it is, the target file (script or LaTeX input), which environment it should run in, and any additional input dependencies or outputs. Let's start with data processing:</p> <pre><code>calkit new python-script-stage \\\n    --name process-data \\\n    --environment py \\\n    --script-path scripts/process.py \\\n    --input data/raw \\\n    --output data/processed\n</code></pre> <p>This will add a stage to the <code>calkit.yaml</code> file that looks like:</p> <pre><code>pipeline:\n  stages:\n    process-data:\n      kind: python-script\n      script_path: scripts/process.py\n      environment: py\n      inputs:\n        - data/raw\n      outputs:\n        - data/processed\n</code></pre> <p>This stage can also be modified later, e.g., if there end up being additional inputs (files or folders which if changed, require the script to be rerun). See the pipeline documentation for more information about defining pipeline stages.</p> <p>Next, create a stage for plotting:</p> <pre><code>calkit new python-script-stage \\\n    --name plot \\\n    --environment py \\\n    --script scripts/plot.py \\\n    --input data/processed \\\n    --input data/raw \\\n    --output figures\n</code></pre> <p>Then add stages to build our LaTeX documents:</p> <pre><code>calkit new latex-stage \\\n    --name build-aps-slides \\\n    --environment tex \\\n    --target pubs/2025-aps-dfd-slides/slides.tex \\\n    --input figures\n</code></pre> <pre><code>calkit new latex-stage \\\n    --name build-article-1 \\\n    --environment tex \\\n    --target pubs/2025-article-1/paper.tex \\\n    --input figures\n</code></pre> <p>If you have other kinds of stages, e.g., MATLAB, R, or shell scripts to run, see the output of <code>calkit new --help</code> for information on how to create those.</p>"},{"location":"tutorials/existing-project/#check-that-the-pipeline-runs-and-push-outputs-to-the-cloud","title":"Check that the pipeline runs and push outputs to the cloud","text":"<p>Now that the pipeline is built, we can check that it runs properly by calling:</p> <pre><code>calkit run\n</code></pre> <p>If there are no errors, we can commit the outputs and push them up to the cloud with <code>calkit save</code>:</p> <pre><code>calkit save -am \"Run pipeline\"\n</code></pre>"},{"location":"tutorials/existing-project/#declare-all-of-the-project-artifacts","title":"Declare all of the project artifacts","text":"<p>Project artifacts like datasets, figures, and publications are declared in the <code>calkit.yaml</code> file. The purpose of doing this is to make them more easily searchable and reusable. For example, users can run <code>calkit import dataset</code> in their own project to reuse one of yours, and your project will be listed as the source in that project's <code>calkit.yaml</code> file. See the FAIR principles to learn more about why this is important.</p> <p>Note that when they are ready for public consumption, we can create a \"release\" that will archive these materials to a service like Figshare, Zenodo, or OSF, and give them a digital object identifier (DOI) for citation and traceability. It's a good idea to create a release of the project before submitting a journal article and to cite it therein, so readers can find their way back to the project and inspect how the materials were created.</p> <p>Let's go ahead an add our raw and processed datasets to <code>calkit.yaml</code>:</p> <pre><code>datasets:\n  - path: data/raw\n    title: Raw data\n  - path: data/processed\n    title: Processed data\n</code></pre> <p>We can add more metadata about each dataset, e.g., a description, or definitions for the columns, but at the very least we need to define a path and title.</p> <p>Next, add the figures to <code>calkit.yaml</code>. This will make them show up in the figures section of the project homepage on calkit.io.</p> <pre><code>figures:\n  - path: figures/plot1.png\n    title: Plot of something\n    description: This is a plot of something.\n    stage: plot\n  - path: figures/plot2.png\n    title: Plot of something else\n    description: This is a plot of something else.\n    stage: plot\n</code></pre> <p>You'll notice we've defined the pipeline stage that produced each of these figures. This will allow users to trace back from the figure to the code that produced it.</p> <p>Lastly, let's add our publications to <code>calkit.yaml</code>, which will make them viewable on the project publications page on calkit.io:</p> <pre><code>publications:\n  - path: pubs/2025-aps-dfd-slides/slides.pdf\n    kind: presentation\n    title: This is the title of the talk\n    stage: build-aps-slides\n  - path: pubs/2025-article-1/paper.pdf\n    kind: journal-article\n    title: This is the title of the paper\n    stage: build-article-1\n  - path: pubs/thesis/thesis.pdf\n    kind: phd-thesis\n    title: This is the title of the thesis\n    stage: build-thesis\n</code></pre> <p>We can then commit and push the changes to <code>calkit.yaml</code> with:</p> <pre><code>calkit save calkit.yaml -m \"Add artifacts to calkit.yaml\"\n</code></pre>"},{"location":"tutorials/existing-project/#next-steps","title":"Next steps","text":"<p>Now that our project is fully version-controlled and reproducible, we have a solid baseline to return to if anything breaks due to future changes. Maybe we have some new figures to generate, or maybe we have a new idea for a derived dataset we can create. A good way to go about doing this is to create a scratch script or notebook, ignoring it with <code>calkit ignore</code>, prototyping in that scratch space, and moving any valuable code out into a version-controlled script once it works the way you want it to.</p> <p>After producing a new working script, add a new pipeline stage to run that script with <code>calkit new</code>. If you need a different environment, you can create one, or you can update an existing environment by editing its definition file. If you execute <code>calkit run</code> again, only the stages that are missing outputs or have updated dependencies will be executed, ensuring the project remains reproducible as efficiently as possible. If you continue to commit all changes along the way, you'll always be able to get back to something that works if something goes wrong, sort of like climbing with a safety harness, clipping it onto higher and higher anchors as you ascend.</p>"},{"location":"tutorials/existing-project/#questions-or-comments","title":"Questions or comments?","text":"<p>Participate in the discussion here.</p>"},{"location":"tutorials/first-project/","title":"\ud83c\udfa5 Creating your first project","text":""},{"location":"tutorials/first-project/#creating-your-first-project","title":"\ud83c\udfa5 Creating your first project","text":""},{"location":"tutorials/github-actions/","title":"Running Calkit in GitHub Actions","text":""},{"location":"tutorials/github-actions/#running-calkit-in-github-actions","title":"Running Calkit in GitHub Actions","text":"<p>A project can be set up to automatically run the pipeline every time a change is pushed, either to the main branch or on a pull request, using GitHub Actions. The latter allows for inspection of outputs before merging into main.</p> <p>To get started, generate a DVC token and set it in your GitHub Actions secrets as <code>CALKIT_DVC_TOKEN</code>, either at your account or project level. On calkit.io, there are shortcuts on the project page for managing both Calkit tokens and GitHub Actions secrets:</p> <p></p> <p></p> <p></p> <p>Next, add a workflow to the project in the <code>.github/workflows</code> folder. For example, we can put the content below into <code>.github/workflows/run.yml</code>:</p> <pre><code>name: Run pipeline\n\non:\n  push:\n    branches:\n      - main\n  pull_request:\n\npermissions:\n  contents: write\n\n# Make sure we only ever run one per branch so we don't have issues pushing\n# after running the pipeline\nconcurrency:\n  group: calkit-run-${{ github.ref }}\n  cancel-in-progress: false\n\njobs:\n  main:\n    name: Run\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          # For PRs, checkout the head ref to avoid detached HEAD\n          ref: ${{ github.head_ref || github.ref_name }}\n          token: ${{ secrets.GITHUB_TOKEN }}\n      - name: Configure Git credentials\n        run: |\n          git config user.name github-actions[bot]\n          git config user.email 41898282+github-actions[bot]@users.noreply.github.com\n      - name: Setup uv\n        uses: astral-sh/setup-uv@v5\n      - name: Install Calkit\n        run: uv tool install calkit-python\n      - name: Run Calkit\n        uses: calkit/run-action@v1\n        with:\n          dvc_token: ${{ secrets.CALKIT_DVC_TOKEN }}\n</code></pre> <p>This particular example installs Calkit with uv, meaning <code>uv</code> and <code>uv-venv</code> environment types will work without any additional configuration. Docker is also installed by default on the <code>ubuntu-latest</code> machine. If other environment types are used in the project, setup steps may be necessary for those, e.g., <code>setup-miniconda</code> or <code>install-juliaup</code>.</p> <p>By default, the Calkit GitHub Action will run the pipeline and save results. This is why the workflow needs write permissions and configures Git credentials to act as the GitHub Actions bot. The workflow also limits concurrency so multiple jobs don't attempt to push to the same branch at the same time.</p> <p>It's possible to configure the action to not save results, e.g., if you just want to check that the pipeline can run without errors. See the documentation for all available options.</p>"},{"location":"tutorials/jupyterlab/","title":"\ud83c\udfa5 Composing a pipeline from multiple Jupyter notebooks in JupyterLab","text":""},{"location":"tutorials/jupyterlab/#composing-a-pipeline-from-multiple-jupyter-notebooks-in-jupyterlab","title":"\ud83c\udfa5 Composing a pipeline from multiple Jupyter notebooks in JupyterLab","text":"<p>The video below explains how to use the Calkit JupyterLab extension to create a single-button reproducible pipeline from a collection of Jupyter notebooks, including automated environment management and output tracking.</p>"},{"location":"tutorials/latex-codespaces/","title":"Collaborating on a LaTeX document with GitHub Codespaces","text":""},{"location":"tutorials/latex-codespaces/#collaborating-on-a-latex-document-with-github-codespaces","title":"Collaborating on a LaTeX document with GitHub Codespaces","text":"<p>Research projects often involve producing some sort of LaTeX document, e.g., a conference paper, slide deck, journal article, proposal, or multiple of each. Collaborating on one of these can be painful, though there are web- or cloud-based tools to help, the most popular of which is probably Overleaf. Overleaf is pretty neat, but the free version is quite limited in terms of versioning, collaboration, and offline editing. Most importantly, I feel like it's only really suited to pure writing projects. Research projects involve writing for sure, but they also involve (often iteratively) collecting and analyzing data, running simulations, creating figures, etc., which are outside Overleaf's scope.</p> <p>Calkit on the other hand is a research project framework encompassing all of the above, including writing, and is built upon tools that can easily run both in the cloud and locally, on- or offline, for maximum flexibility. Here we're going to focus doing everything in a web browser though. We'll set up a collaborative LaTeX editing environment with Calkit and GitHub Codespaces, a container-based virtual machine service.</p>"},{"location":"tutorials/latex-codespaces/#create-the-project","title":"Create the project","text":"<p>In order to follow along, you'll need a GitHub account, so if you don't have one, sign up for free. Then head to calkit.io, sign in with GitHub, and click the \"create project\" button. Upon submitting, Calkit will create a new GitHub repository for us, setup DVC (Data Version Control) inside it, and create a so-called \"dev container\" configuration from which we can spin up our GitHub Codespace and start working.</p> <p></p>"},{"location":"tutorials/latex-codespaces/#add-a-new-publication-to-the-project","title":"Add a new publication to the project","text":"<p>Click the quick action link on the project homepage to \"create a new publication from a template.\" In the dialog, select the <code>latex/article</code> template, and fill in the rest of the required information. This will add a LaTeX article to our repo and a build stage to our DVC pipeline, which will automatically create a TeX Live Docker environment to build the document. Here we'll create the document in a new folder called <code>paper</code>:</p> <p></p> <p>Keep in mind that you'll be able add different LaTeX style and config files later on if the generic article template doesn't suit your needs. Also, if you have suggestions for templates you think should be included, drop a note in a new GitHub issue.</p>"},{"location":"tutorials/latex-codespaces/#create-the-codespace","title":"Create the Codespace","text":"<p>From the project homepage, click \"Open in GitHub Codespaces.\" Alternatively, if you haven't created your own project, you can create your own Codespace in mine.</p> <p>Once created, we'll see an in-browser Visual Studio Code (VS Code) editor, which will have access to our project repository and will be able to compile the LaTeX document. Consider this your very own Linux virtual machine in the cloud for working on this project. You can update settings, add extensions, etc. You have total control over it. Note that GitHub does charge for Codespaces, but the free plan limits are reasonably generous. It's also fairly easy to run the same dev container configuration locally in in VS Code.</p> <p>It might take few minutes to start up the first time as the Codespace is created, so go grab a coffee or take the dog for a walk.</p> <p></p>"},{"location":"tutorials/latex-codespaces/#edit-and-build-the-document","title":"Edit and build the document","text":"<p>After the Codespace is built and ready, we can open up <code>paper/paper.tex</code> and start writing.</p> <p></p> <p>If you look in the upper right hand corner of the editor panel, you'll see a play button icon added by the LaTeX Workshop extension. Clicking that will rebuild the document. Just to the right of that button is one that will open the PDF in split window, which will refresh on each build.</p> <p></p> <p>Note that the play button will run the entire pipeline (like calling <code>calkit run</code> from a terminal,) not just the paper build stage, so we can add more stages later, e.g., for creating figures, or even another LaTeX document, and everything will be kept up-to-date as needed. This is a major difference between this workflow and that of a typical LaTeX editor, i.e., that the entire project is treated holistically. So for example, there's no need to worry about if you forgot to rerun the paper build after tweaking a figure---it's all one pipeline. See this project for an example, and check out the DVC stage documentation for more details on how to build and manage a pipeline.</p>"},{"location":"tutorials/latex-codespaces/#break-lines-in-a-git-friendly-way","title":"Break lines in a Git-friendly way","text":"<p>This advice is not unique to cloud-based editing, but it's worth mentioning anyway. When writing documents that will be versioned with Git, make sure to break lines properly by splitting them at punctuation or otherwise breaking into one logical phrase per line. This will help when viewing differences between versions and proposed changes from collaborators. If you write paragraphs as one long line and let them \"soft wrap,\" it will be a little more difficult.</p> <p>So, instead of writing something like:</p> <pre><code>This is a very nice paragraph. It consists of many sentences, which make up the paragraph.\n</code></pre> <p>write:</p> <pre><code>This is a very nice paragraph.\nIt consists of many sentences,\nwhich make up the paragraph.\n</code></pre> <p>The compiled document will look the same.</p>"},{"location":"tutorials/latex-codespaces/#commit-and-sync-changes","title":"Commit and sync changes","text":"<p>For better or for worse, working with Git/GitHub is different from other systems like Google Docs, Overleaf, or Dropbox. Rather than syncing our files automatically, we need to deliberately \"commit\" changes to create a snapshot and then sync or \"push\" them to the cloud. This can be a stumbling block when first getting started, but one major benefit is that it makes one stop and think about how to describe a given set of changes. Another benefit is that every snapshot will be available forever, so if you create lots of them, you'll never lose work. In a weird mood and ruined a paragraph that read well yesterday? Easy fix---just revert the changes.</p> <p>The VS Code interface has a built-in graphical tool for working with Git and GitHub, which can make things a little easier compared to learning the command-line interface (CLI.) If we make some changes to <code>paper.tex</code>, we can see a blue notification dot next to the source control icon in the left sidebar. In this view we can see there are two files that have been changed, <code>paper.tex</code> and <code>dvc.lock</code>, the latter of which is a file DVC creates to keep track of the pipeline, and shows up in the \"Staged Changes\" list, a list of files that would be added to a snapshot if we were to create a commit right now.</p> <p>We want to save the changes both this file and <code>paper.tex</code> in one commit, so let's stage the changes to <code>paper.tex</code>, write a commit message, and click commit.</p> <p></p> <p>After committing we'll see a button to sync the changes with the cloud, which we can go ahead and click. This will first pull from and then push our commits up to GitHub, which our collaborators will then be able to pull into their own workspaces.</p>"},{"location":"tutorials/latex-codespaces/#push-the-pdf-to-the-cloud","title":"Push the PDF to the cloud","text":"<p>The default behavior of DVC is to not save pipeline outputs like our compiled PDF to Git, but instead commit them to DVC, since Git is not particularly good at handling large and/or binary files. The Calkit Cloud serves as a \"DVC remote\" for us to push these artifacts to back them up and make them available to others with access to the project.</p> <p>If we go down to the terminal and run <code>calkit push</code>, we'll push our DVC artifacts (just the PDF at this point) up to the cloud as well, which will make our PDF visible in the publications section of the project homepage. Note that <code>calkit push</code> will also send the Git changes to GitHub, completely backing up the project.</p> <p></p> <p>Later on, if you end up adding things like large data files for analysis, or even photos and videos from an experiment, these can also be versioned with DVC and backed up in the cloud.</p>"},{"location":"tutorials/latex-codespaces/#collaborate-concurrently","title":"Collaborate concurrently","text":"<p>What we've seen so far is mostly an individual's workflow. But what if we have multiple people working on the document at the same time? Other cloud-based systems like Google Docs and Overleaf allow multiple users to edit a file simultaneously, continuously saving behind the scenes. My personal opinion is that concurrent collaborative editing is usually not that helpful, at least not on the same paragraph(s). However, if you really like the Google Docs experience, you can setup the Codespace for live collaboration. Otherwise, collaborators can create their own Codespaces from the same configuration just like we created ours.</p> <p>Git is actually quite good at automatically merging changes together, but typically you'll want to make sure no two people are working on the same lines of text at the same time. You'll need to communicate a little more with your collaborators so you don't step on each other's toes and end up with merge conflicts, which require manual fixes. You could simply send your team a Slack message letting them know you're working on the doc, or a given section, and avoid conflicts that way. You could split up the work by paragraph or section, and even use LaTeX <code>\\input</code> commands in the main <code>.tex</code> file to allow each collaborator to work on their own file.</p> <p>Git can also create different branches of the repo in order to merge them together at a later time, optionally via GitHub pull requests, which can allow the team to review proposed changes before they're incorporated. However, for many projects, it will be easier to have all collaborators simply commit to the main branch and continue to clean things up as you go. If commits are kept small with descriptive messages, this will be even easier. Also, be sure to run <code>git pull</code> often, either from the UI or from the terminal, so you don't miss out on others' changes.</p>"},{"location":"tutorials/latex-codespaces/#manage-the-project-with-github-issues","title":"Manage the project with GitHub issues","text":"<p>Another important aspect of collaborative writing is reviewing and discussing the work. I recommend using GitHub issues as a place to create to-do items or tasks and discuss them, which is particularly helpful for team members who are mostly reviewing rather thant writing.</p> <p>One approach to creating issues is to Download the latest PDF of the document, add comments, and attach the marked up PDF to a new GitHub issue. A variant of this is printing it out and scanning the version with red pen all over it.</p> <p>Another approach is to create issues from within VS Code. In the <code>.tex</code> file, you can highlight some text and create a GitHub issue from it with the \"Create Issue From Selection\" command. Open up the command palette with <code>ctrl/cmd+shift+p</code> and start typing \"issue from selection\". The command should show up at the top of the list.</p> <p></p> <p>After you create a new issue, click the GitHub icon in the left pane and look through the recent issues. You can right click on an issue and select \"Go to Linked Code\" to do just that.</p> <p></p> <p>If you make a commit that addresses a given issue, you can include \"fixes #5\" or \"resolves #5\" in the commit message, referencing the issue number, and GitHub will automatically close it.</p> <p>For complex cases with lots of tasks and team members, GitHub projects is a nice solution, allowing you to put your tasks into a Kanban board or table, prioritize them, assess their effort level, and more. Also note that these GitHub issues will also show up in the \"To-do\" section on the Calkit project homepage, and can be created and closed from there as well.</p>"},{"location":"tutorials/latex-codespaces/#conclusions","title":"Conclusions","text":"<p>Here we set up a way to collaborate on a LaTeX document in the cloud using GitHub Codespaces. The process was a little more involved compared to using a dedicated LaTeX web app like Overleaf, but our assumption was that this document is part of a larger research project that involves more than just writing. Because the document is integrated into a Calkit project, it is built as a stage in a DVC pipeline, which can later be extended to include other computing tasks like creating datasets, processing them, making figures, and more.</p> <p>We also went over some tactics to help with version control, concurrent editing, and project management. Though we did everything in a web browser, this setup is totally portable. We'll be able to work equally well locally as we can in the cloud, allowing anyone to reproduce the outputs anywhere.</p>"},{"location":"tutorials/matlab/","title":"\ud83c\udfa5 Using MATLAB","text":""},{"location":"tutorials/matlab/#using-matlab","title":"\ud83c\udfa5 Using MATLAB","text":""},{"location":"tutorials/notebook-pipeline/","title":"Using a Jupyter Notebook as a reproducible pipeline","text":""},{"location":"tutorials/notebook-pipeline/#using-a-jupyter-notebook-as-a-reproducible-pipeline","title":"Using a Jupyter Notebook as a reproducible pipeline","text":"<p>Jupyter Notebooks are great tools for exploration and prototyping, but they can be troublesome if relied upon to produce permanent artifacts like figures, datasets, or machine learning models. Their strength for ad hoc work is their weakness for \"production\" work, namely that their cells can be executed in any order, and they can be difficult to use with Git, hindering their reproducibility. Furthermore, expensive cells may inspire home grown caches that can be cumbersome to invalidate or share between collaborators.</p> <p>It's typically recommended to move anything important or production-ready out of notebooks and into modules and/or scripts so they can be easily version-controlled and run as part of a reproducible pipeline. However, Calkit includes a Jupyter cell magic to help \"productionize\" notebook cells as DVC pipeline stages without needing to cut/paste anything.</p> <p>This enables a workflow like:</p> <ol> <li>Prototype a cell by running whatever commands make sense.</li> <li>Convert cells that are working and valuable into pipeline    stages, and delete anything else.</li> </ol> <p>In the process of making notebook cells into pipeline stages, we will need to be explicit about what variables our cells depend on and which are outputs, since the cells will be executed outside of out Jupyter kernel in a separate process. Those processes won't have access to any state that isn't declared as a dependency or created by the code itself, thereby negating some of the state management traps one can run into if running cells out of order, changing cells but forgetting to rerun them, etc.</p> <p>At the end of this process we should be left with a notebook that runs very quickly after it's been run once, and all of our important outputs will be cached and pushed to the cloud, but kept out of our Git repo. Our collaborators will be able to pull everything and similarly run the notebook very quickly on the first go, and if/when cells are changed, DVC will only rerun what is necessary to rerun.</p> <p>Side note: We should also be using <code>nbstripout</code> to strip notebook outputs before we commit to the repo, since the important ones will be produced as part of the pipeline and cached with DVC.</p> <p>Now let's work through an example. First, we'll write a cell to simulate fetching a dataset. To simulate this being an expensive call, e.g., if we had to query a database, we'll use a call to <code>time.sleep</code>.</p> <pre><code>import pandas as pd\nimport time\n\ntime.sleep(10)\n\ndf = pd.DataFrame({\"col1\": range(1000)})\ndf.describe()\n</code></pre> <p>In order to convert this cell into a pipeline stage, we'll need to load the Calkit magics in our notebook. This only needs to be run once, so it can be at the very top:</p> <pre><code>%load_ext calkit.magics\n</code></pre> <p>Next we simply call the <code>%%stage</code> magic with the appropriate arguments to convert the cell into a pipeline stage and run it externally with DVC:</p> <pre><code>%%stage --name get-data --environment main --out df\n\nimport pandas as pd\nimport time\n\ntime.sleep(10)\n\ndf = pd.DataFrame({\"col1\": range(1000)})\ndf.describe()\n</code></pre> <p>In the magic call, we gave the stage a name, specified the environment in which it should run, and declared an output <code>df</code>. When we run the cell, we'll see it takes at least 10 seconds the first time, but if we run it a second time, it will be much faster, since our output is being fetched from the DVC cache. If we run <code>calkit status</code>, we can see we have some new data to commit and push to the DVC remote. If we do that, anyone else who clones this project will be able to pull in the cache, and the cell will run quickly for them.</p>"},{"location":"tutorials/notebook-pipeline/#saving-outputs-in-different-formats","title":"Saving outputs in different formats","text":"<p>By default, our output variables will be pickled, which is not the most portable format. Let's instead save our DataFrame to Parquet format. To do this, all we need to do is adjust the <code>--out</code> value to add the format and DataFrame library (Calkit currently supports both Pandas and Polars DataFrames.) So change the call to the magic to be:</p> <pre><code>%%stage --name get-data --environment main --out df:parquet:pandas\n</code></pre>"},{"location":"tutorials/notebook-pipeline/#using-the-output-of-one-cell-as-a-dependency-in-another","title":"Using the output of one cell as a dependency in another","text":"<p>Let's imagine that now we want to create a visualization of our data. Just like if we were creating a typical DVC stage in a <code>dvc.yaml</code> file, we can declare a cell to depend on the output of another cell with the <code>--dep</code> command. For example:</p> <pre><code>%%stage --name plot --environment main --dep get-data:df:parquet:pandas --out fig\n\nfig = df.plot(backend=\"plotly\")\nfig\n</code></pre> <p>In this case, we need to specify what DataFrame library to use to read in this dependency. Here we tell Calkit that it's a Parquet file to be read with Pandas. Calkit will ensure this dependency is loaded into memory before running the cell as part of the pipeline.</p>"},{"location":"tutorials/notebook-pipeline/#declaring-an-output-as-a-figure-saved-to-a-different-path","title":"Declaring an output as a figure saved to a different path","text":"<p>In the cell above we end up pickling <code>fig</code> into the DVC cache, which is fine if we only ever want to view the figure through the notebook interface, but what if we want to declare this as a figure and, e.g., use it in a publication? We can add a line that saves the figure and declare an additional output path and metadata like (note this requires <code>plotly</code> and <code>kaleido</code> to be installed):</p> <pre><code>%%stage \\\n   --name plot \\\n   --environment main \\\n   --dep get-data:df:parquet:pandas \\\n   --out fig \\\n   --out-path figures/plot.png \\\n   --out-type figure \\\n   --out-title \"A plot of the data\" \\\n   --out-desc \"This is a plot of the data.\"\n\nimport os\n\nos.makedirs(\"figures\", exist_ok=True)\n\nfig = df.plot(backend=\"plotly\")\nfig.write_image(\"figures/plot.png\")\nfig\n</code></pre> <p>If we call <code>calkit list figures</code>, we'll see our figure, and after pushing to the cloud, we'll be able to see it there as well.</p> <p>Note that we could also go back and add <code>--out-type=dataset</code> to the <code>get-data</code> cell, which will similarly add that dataset to our project metadata for searchability and reuse.</p>"},{"location":"tutorials/notebook-pipeline/#running-the-pipeline-outside-the-notebook","title":"Running the pipeline outside the notebook","text":"<p>To incorporate the notebook into the project pipeline in <code>calkit.yaml</code>, e.g., if it's called <code>notebook.ipynb</code>, it can be added like:</p> <pre><code># In calkit.yaml\nenvironments:\n  main:\n    kind: venv # Use python's built in venv module\n    path: requirements.txt\n    prefix: .venv\npipeline:\n  stages:\n    run-notebook:\n      kind: jupyter-notebook\n      notebook_path: notebook.ipynb\n      environment: main\n      html_storage: dvc\n</code></pre> <p>Executing <code>calkit run</code> will then execute the notebook, and any cached cell stages will not be rerun, but their outputs will make there way back into the notebook. Lastly, an HTML version of the notebook will be exported and versioned with DVC (though the storage location is configurable).</p>"},{"location":"tutorials/notebook-pipeline/#further-exploration","title":"Further exploration","text":"<p>If you'd like to try this out or explore further, you can view this project up on GitHub or the Calkit cloud.</p>"},{"location":"tutorials/office/","title":"Using Microsoft Office","text":""},{"location":"tutorials/office/#using-microsoft-office","title":"Using Microsoft Office","text":"<p>Note</p> <p>Many of the steps shown here use the Calkit web UI. However, all steps could have just as easily been performed with the CLI.</p> <p></p> <p>Does using Microsoft Office always mean going to the dark side?</p> <p>Everyone knows that when you want to get serious about reproducibility you need to stop using Microsoft Word and Excel and become a computer hacker, right? There is some truth to that, that working with simpler, open source, less interactive tools is typically better for producing permanent artifacts and enabling others to reproduce them, but it's not mandatory.</p> <p>On the other hand, it's starting to become more and more common, and will hopefully someday be mandatory to share all code and data when submitting a manuscript to a journal, so that others can reproduce the results. This is a good thing for science overall, but also good for individual researchers, even though it may seem like more work.</p> <p>Besides the fact that you'll probably get more citations, which should not necessarily be a goal in and of itself given recent controversies around citation coercion, working reproducibly will keep you more organized and focused, and will allow you to produce higher quality work more quickly. I would also be willing to bet that if reviewers can reproduce your work, your paper will get through the review process faster, shortening the time-to-impact.</p> <p></p> <p>Elsevier's research data availability standards (with annotations.)</p> <p>Here we'll see that it's possible to get started working reproducibly without becoming a de facto software engineer, that it's okay to use whatever tools you prefer so long as you follow the relevant principles. Inspired by the article Ten Simple Rules for Computational Research, we're going focus on just two:</p> <ol> <li>Keep all files in version control.    This means a real version control system.    Adding your initials and a number to the filename    is kind of like a version control system, but is messy and error-prone.    It should be easy to tell if a file has been modified since the last time    is was saved.    When you make a change you should have to describe that change,    and that record should exist in the log forever.    When all files are in a true version-controlled repository, it's like using    \"track changes\" for an entire folder,    and it doesn't require any discipline to avoid    corrupting the history, e.g., by changing a version after it has had its    filename suffix added.</li> <li>Generate permanent artifacts with a pipeline.    Instead of a bunch of manual steps, pointing and clicking,    we should be able to repeatedly perform the same single action over and over    and get the same output.    A good pipeline will allow us to know if our output artifacts, e.g., figures,    derived datasets, papers,    have become out-of-date and no longer reflect their input data or    processing procedures, after which we can run the pipeline and get them    up-to-date.    It also means we only need to focus on building that pipeline and running    it. We don't need to memorize what steps to perform in what order---just    run the pipeline.</li> </ol> <p></p> <p>Manual or ad hoc 'version control' (don't do this.) From phdcomics.com.</p> <p>In order to follow rule number 1, we are going to treat our project's repository, or \"repo,\" as the one place to store everything. Any file that has anything to do with our work on this project goes in the repo. This will save us time later because there will be no question about where to look for stuff, because the answer is: in the repo.</p> <p>This repo will use Git for text-based files and DVC for binary files, e.g., our Excel spreadsheets and Word documents. Don't worry though, we're not actually going to interact with Git and DVC directly. This is a major sticking point for some people, and we totally sympathize. Learning Git is a daunting task. However, here all the Git/DVC stuff will be done for us behind the scenes.</p> <p>We can start off by creating a Git (and GitHub) repo for our project up on calkit.io.</p> <p></p> <p>Creating the project on calkit.io.</p> <p>Next, we'll do the only command line thing in this whole process and spin up a local Calkit server. This will allow us connect to the web app and enable us to modify the project on our local machine. To start the server, open up a terminal or Miniforge command prompt and run:</p> <pre><code>calkit local-server\n</code></pre> <p>If we navigate to our project page on calkit.io, then go to the local machine page, we see that the repo has never been cloned to our computer, so let's click the \"Clone\" button.</p> <p></p> <p>The project's local machine page showing we haven't cloned the repo yet.</p> <p>By default, it will be cloned somewhere like <code>C:/Users/your-name/calkit/the-project-name</code>, which you can see in the status. We can also see that our repo is \"clean,\" i.e., there are no untracked or modified files in there, and that our local copy is synced with both the Git and DVC remotes, meaning everything is backed up and we have the latest version. We'll strive to keep it that way.</p> <p>Now that we have our repository cloned locally let's create our dataset. We are going to do this by adding some rows to an Excel spreadsheet and saving it in our project repo as <code>data.xlsx</code>.</p> <p></p> <p>Creating our dataset in Excel.</p> <p>Back on the Calkit local machine page, if we refresh the status we see that the <code>data.xlsx</code> spreadsheet is showing up as an untracked file in the project repo. Let's add it to the repo by clicking the \"Add\" button.</p> <p></p> <p>We have an untracked file.</p> <p>After adding and committing, Calkit is going to automatically push to the remotes so everything stays backed up, and again we'll see that our repo is clean and in sync.</p> <p>Now let's use Excel to create a figure. If we go in and create a chart inside and save the spreadsheet, we see up on the local machine page that we have a changed file. Let's commit that change by clicking the \"Commit\" button and let's use a commit message like \"Add chart to spreadsheet\".</p> <p></p> <p>Creating our figure in Excel.</p> <p></p> <p>Uncommitted changes in the repo after adding a chart to the spreadsheet.</p> <p>At this point our data is in version control so we'll know if it ever changes. Now it's time for rule number 2: Generate important artifacts with a pipeline. At the moment our pipeline is empty, so let's create a stage that extracts our chart from Excel into an image and denotes it as a figure in the project. On the web interface we see there's a button to create a new stage, and in there are some optional stage templates. If we select \"Figure from Excel\", there will be a few fields to fill out:</p> <ol> <li>The name of the stage. We'll use <code>extract-chart</code>, but you can call it    whatever you like.</li> <li>The Excel file path relative to the repo (<code>data.xlsx</code>).</li> <li>The desired output file path for our image. We'll use <code>figures/chart.png</code>,    but again, you can choose whatever makes sense to you.</li> <li>The title and description of our figure.</li> </ol> <p></p> <p>Creating a new pipeline stage to extract our chart from Excel.</p> <p>After saving the stage and refreshing the status we'll see that the pipeline is out-of-date, which makes sense. We added a stage but haven't yet run the pipeline. So let's do that by clicking the \"Run\" button.</p> <p></p> <p>The pipeline is out-of-date after adding a stage.</p> <p>After the pipeline has been run we can see there are some uncommitted changes in the repo, so let's commit them with a message that makes sense, e.g., \"Extract figure from data.xlsx\". We should again be in our happy state, with a clean repo synced with the cloud, and a pipeline that's up-to-date.</p> <p>To wrap things up, we're going to use this figure in a paper, written using Microsoft Word. So, find a journal with a Microsoft Word (<code>.docx</code>) submission template, download that, and save it in the repo. In this case, I saved the IOP Journal of Physics template generically as <code>paper.docx</code>, since in the context of this project, it doesn't need a special name, unless of course <code>paper.docx</code> would somehow be ambiguous. We can then follow the same process we followed with <code>data.xlsx</code> to add and commit the untracked <code>paper.docx</code> file to the repo.</p> <p>Now let's open up the Word document and insert our PNG image exported from the pipeline. Be sure to use the \"link to file\" or \"insert and link\" option, so Word doesn't duplicate the image data inside the document. This will allow us to update the image externally and not need to reimport into Word.</p> <p></p> <p>Linking to the image file when inserting a picture in Word.</p> <p>Again when we refresh we'll see that <code>paper.docx</code> has uncommitted changes, so let's commit them with a message like \"Add figure to paper\".</p> <p>Now let's complete our pipeline by adding a stage to convert our Word document to PDF, so that can be the main artifact we share with the outside world. There's a stage template for that on the website, so follow the stage generation steps we used to extract the figure, but this time select the \"Word document to PDF\" template, filling out the Word document file path, the output PDF path, add <code>figures/chart.png</code> to the list of input dependencies, and select \"publication\" as our artifact type. Fill in the title and description of the publication as well. Adding <code>figures/chart.png</code> to the input dependencies will cause our PDF generation stage to be rerun if that or <code>paper.docx</code> changes. Otherwise, it will not run, so running the pipeline can be as fast as possible.</p> <p></p> <p>Adding a stage to convert our Word document to PDF.</p> <p>Again the pipeline will show that it's out-of-date, so let's run and commit again, using a message like \"Export paper to PDF\". If we open up <code>paper.pdf</code> we can see that our figure is there just like we expected.</p> <p>But hold on a second you might say. Why did we go through all this trouble just to create a PDF with an Excel chart in it? This would have been only a few steps to do manually! That would be a valid point if this were a one-off project and nothing about it would ever change. However, for a research project, there will almost certainly be multiple iterations (see again the PhD Comics cartoon above,) and if we need to do each manual step each iteration, it's going to get costly time-wise, and we could potentially forget which steps need to be taken based on what files were changed. We may end up submitting our paper with a chart that doesn't reflect the most up-to-date data, which would mean the chart in the paper could not be reproduced by a reader. Imagine if you had multiple datasets, multiple steps of complex data analysis, a dozen figures, and some slides to go along with your paper. Keeping track of all that would consume valuable mental energy that could be better spent on interpreting and communicating the results!</p> <p></p> <p>A diagram and YAML representation of the pipeline on calkit.io.</p> <p>To close the loop and show the value of using version control and a pipeline, let's go and add a few rows to our dataset, which will in turn change our chart in Excel. If we save the file and look at the status, we can see that this file is different, and that our pipeline is again out-of-date, meaning that our primary output (the PDF of the paper) not longer reflects our input data.</p> <p></p> <p>Adding rows to the dataset.</p> <p></p> <p>After adding a row to the spreadsheet, the pipeline is again out-of-date.</p> <p>Now with one click we can rerun the pipeline, which is going to update both our figure PNG file and the paper PDF in one shot. We can then create a commit message explaining that we added to the dataset. These messages can be audited later to see when and why something changed, which can come in handy if all of a sudden things aren't looking right. Having the files in version control also means we can go check out an old version if we made a mistake.</p> <p></p> <p>Confirming the figure in our publication's PDF includes the additional rows.</p> <p>Well, we did it. We created a reproducible workflow using Microsoft Word and Excel, and we didn't need to learn Git or DVC or become a command line wizard. Now we can iterate on our data, analysis, figures, and writing, and all we need to do to get them all up-to-date and backed up is to run the pipeline and commit any changes. Now we can share our project and others can reproduce the outputs (so long as they have a Microsoft Office license, but that's a topic for another post.) Everything is still in Git and DVC, so our more command line-oriented colleagues can work in that way if they like using the same project repo. To achieve this, all we had to do was follow the two most important rules:</p> <ol> <li>All files go in version control.</li> <li>Artifacts need to be generated by the pipeline.</li> </ol> <p>If you'd like you can browse through this project up on calkit.io.</p>"},{"location":"tutorials/openfoam/","title":"OpenFOAM, Docker, and Reviewer 2","text":""},{"location":"tutorials/openfoam/#openfoam-docker-and-reviewer-2","title":"OpenFOAM, Docker, and Reviewer 2","text":""},{"location":"tutorials/openfoam/#the-problem","title":"The problem","text":"<p>Have you ever been here before? You've done a bunch of work to get a simulation to run, created some figures, and submitted a paper to a journal. A month or two later you get the reviews back and you're asked by the dreaded Reviewer 2 to make some minor modifications to a figure. There's one small problem, however: You don't remember how that figure was created, or you've upgraded your laptop and now the script won't run. Maybe you were able to clone the correct Git repo with the code, but you don't remember where the data is supposed to be stored. In other words, your project is not reproducible.</p> <p>Here we'll show how to fix that.</p>"},{"location":"tutorials/openfoam/#creating-and-cloning-the-project-repo","title":"Creating and cloning the project repo","text":"<p>Head over to calkit.io and log in with your GitHub account. Click the button to create a new project. Let's title ours \"RANS boundary layer validation\", since for our example, we're going to create a project that attempts to answer the question:</p> <p>What RANS model works best for a simple boundary layer?</p> <p>We'll keep this private for now (though in general it's good to work openly.) Note that creating a project on Calkit also creates the project Git repo on GitHub.</p> <p></p> <p>We're going to need a token to interact with the Calkit API, so head over to your user settings, generate one for use with the API, and copy it to your clipboard.</p> <p></p> <p>Then we can set that token in our Calkit configuration with:</p> <pre><code>calkit config set token YOUR_TOKEN_HERE\n</code></pre> <p>Next, clone the repo to your local machine with (filling in your username):</p> <pre><code>calkit clone https://github.com/YOUR_USERNAME/rans-boundary-layer-validation.git\n</code></pre> <p>Note you can modify the URL above to use SSH if that's how you interact with GitHub.</p> <p></p> <p><code>calkit clone</code> is a simple wrapper around <code>git clone</code> that sets up the necessary configuration to use the Calkit Cloud as a DVC remote, the place where we're going to push our data, while our code goes to GitHub.</p>"},{"location":"tutorials/openfoam/#getting-some-validation-data","title":"Getting some validation data","text":"<p>We want to validate these RANS models, so we'll need some data for comparison. It just so happens that there is already a boundary layer direct numerical simulation (DNS) dataset on Calkit downloaded from the Johns Hopkins Turbulence Databases (JHTDB), so we can simply import that with (after <code>cd</code>ing into the project directory):</p> <pre><code>calkit import dataset \\\n    petebachant/boundary-layer-turbulence-modeling/data/jhtdb-transitional-bl/time-ave-profiles.h5 \\\n    data/jhtdb-profiles.h5\n</code></pre> <p>If we run <code>calkit status</code> we can see that there is one commit that has been made but not pushed to <code>origin/main</code> (on GitHub), so we can make sure everything is backed up there with <code>calkit push</code>.</p> <p></p> <p><code>calkit status</code> and <code>calkit push</code> behave very similarly to <code>git status</code> and <code>git push</code>. In fact, those commands are run alongside some additional DVC commands, the importance of which we will see shortly.</p> <p>We can now see our imported dataset as part of the project datasets on the Calkit website. We can also see the file is present, but ignored by Git, since it's managed by DVC. Because the dataset was imported, it does not take up any of this project's storage space, but will be present when the repo is cloned.</p> <p></p>"},{"location":"tutorials/openfoam/#creating-a-reproducible-openfoam-environment-with-docker","title":"Creating a reproducible OpenFOAM environment with Docker","text":"<p>If you've never worked with Docker, it can sound a bit daunting, but Calkit has some tooling to make it a bit easier. Basically, Docker is going to let us create isolated reproducible environments in which to run commands and will keep track of which environments belong to this project in the <code>calkit.yaml</code> file.</p> <p>Let's create an OpenFOAM-based Docker environment with:</p> <pre><code>calkit new docker-env \\\n    --name foam \\\n    --image openfoam-2406-foampy \\\n    --from microfluidica/openfoam:2406 \\\n    --add-layer miniforge \\\n    --add-layer foampy \\\n    --description \"OpenFOAM v2406 with foamPy.\"\n</code></pre> <p>This command will create the necessary Dockerfile and add the environment to our project metadata. The environment will be automatically created or updated as necessary when a command is executed in this environment.</p> <p>If we run <code>calkit xenv -- blockMesh -help</code>, the Docker image build be built and we'll see the help output from <code>blockMesh</code>. Running this command again will not kick off a rebuild unless the Dockerfile is modified.</p> <p>You'll notice there is a new <code>Dockerfile-lock.json</code> file in the repo, which is how Calkit determines if the image needs to be rebuilt. We should now commit and push this file to the cloud so it is backed up and accessible to our collaborators. We can do this with the <code>calkit save</code> command:</p> <pre><code>calkit save Dockerfile-lock.json -m \"Add Docker lock file\"\n</code></pre> <p>which does the <code>add</code>, <code>commit</code>, <code>push</code> steps all in one, deciding which files to store in DVC versus Git, and pushing to the respective locations to save time and cognitive overhead. However, if desired, you can of course run those individually for full control.</p> <p>Now we're good to go. We didn't need to install OpenFOAM, and neither will our collaborators. We're now ready to start setting up the cases.</p>"},{"location":"tutorials/openfoam/#adding-the-simulation-runs-to-the-pipeline","title":"Adding the simulation runs to the pipeline","text":"<p>We can run things interactively and make sure things work, but it's not a good idea to rely on interactive or ad hoc processes to produce a permanent artifact. Any time you get to a point where you do want to save something permanent, the pipeline should be updated to produce that artifact, so let's add some simulation runs to ours.</p> <p>We want to run the simulation to validate a few different turbulence models:</p> <ul> <li>Laminar (no turbulence model)</li> <li>$k$\u2013$\\epsilon$</li> <li>$k$\u2013$\\omega$</li> </ul> <p>I've setup this project to use foamPy to create and run variants of a case with a \"templatized\" <code>turbulenceProperties</code> file via a script <code>run.py</code>, which we're going to run in our Docker environment.</p> <p>To simulate the same case for multiple turbulence models, we're going to create a stage to run our script, iterating over a sequence of values. We can create this stage with:</p> <pre><code>calkit new python-script-stage \\\n    --name run-sim \\\n    --script-path run.py \\\n    --environment foam \\\n    --arg \"--turbulence-model={turb_model}\" \\\n    --arg \"--overwrite\" \\\n    --input system \\\n    --input constant/transportProperties \\\n    --output \"cases/{turb_model}/postProcessing\" \\\n    --iter turb_model laminar,k-epsilon,k-omega \\\n</code></pre> <p>Another call to <code>calkit status</code> shows our pipeline needs to be run, which makes sense, so let's give it another <code>calkit run</code>. You'll note at the very start our Docker image build stage is not rerun thanks to DVC tracking and caching the inputs and outputs.</p> <p>The output of <code>calkit status</code> now shows something we haven't seen yet: we have some new data produced as part of those simulation runs. We can do another <code>calkit save</code> to ensure everything is committed and pushed:</p> <pre><code>calkit save -am \"Run simulations\"\n</code></pre> <p>In this case, the outputs of the simulations were pushed up to the DVC remote in the Calkit Cloud.</p> <p>We are defining an output for each simulation as the <code>postProcessing</code> folder, which we will cache and push to the cloud for backup, so others (including our future self), can pull down the results and work with them without needing to rerun the simulations. We are also defining dependencies for the simulations. What this means is that if anything in the <code>system</code> folder, the <code>run.py</code> script, <code>constant/transportProperties</code>, or our Dockerfile changes, DVC will know it needs to rerun the simulations. Conversely, if those haven't changed and we already have results cached, there's no need to rerun. This is nice because to produce our outputs we basically only need to remember one command and keep running it, and that's simply <code>calkit run</code>, which is a wrapper around <code>dvc repro</code> that parses some additional metadata to define certain special objects, e.g., datasets or figures.</p>"},{"location":"tutorials/openfoam/#creating-a-figure-to-visualize-our-results","title":"Creating a figure to visualize our results","text":"<p>We want to compare the OpenFOAM results to the DNS data, for which we can plot the mean streamwise velocity profiles, for example. We can create a new figure with its own pipeline stage with:</p> <pre><code>calkit new figure \\\n    figures/mean-velocity-profiles.png \\\n    --title \"Mean velocity profiles\" \\\n    --description \"Mean velocity profiles from DNS and RANS models.\" \\\n    --stage plot-mean-velocity-profiles \\\n    --cmd \"calkit xenv python scripts/plot-mean-velocity-profiles.py\" \\\n    --dep scripts/plot-mean-velocity-profiles.py \\\n    --dep data/jhtdb-profiles.h5 \\\n    --deps-from-stage-outs run-sim\n</code></pre> <p>The last line there is going to automatically create dependencies based on the outputs of our <code>run-sim</code> stage, saving us the trouble of typing out all of those directories manually.</p> <p>Another call to <code>calkit status</code> shows we need to run the pipeline again, and another call to <code>calkit run</code> then <code>calkit save -m \"Run pipeline to create figure\"</code> will create this figure and push it to the repo. This figure is now viewable as its own object up on the website:</p> <p></p>"},{"location":"tutorials/openfoam/#solving-the-problem","title":"Solving the problem","text":"<p>Now let's show the value of making our project reproducible, addressing the problem we laid out in the introduction, assuming Reviewer 2's request was something like:</p> <p>The legend labels should be updated to use mathematical symbols.</p> <p>We're going to pretend we were forced to start from scratch, so let's delete the local copy of our project repo and clone it again using same <code>calkit clone</code> command we ran above.</p> <p></p> <p>After moving into the freshly cloned repo, you'll notice our imported dataset, the <code>cases/*/postProcessing</code> directories, and our figure PNG file were all downloaded, which would not have happened with a simple <code>git clone</code>, since those files are kept out of Git.</p> <p>Running <code>calkit status</code> and <code>calkit run</code> again shows that what we've cloned is fully up-to-date.</p> <p>Next we edit our plotting script to make the relevant changes. Then we execute <code>calkit run</code>. Again, notice how the simulations were not rerun thanks to the DVC cache.</p> <p>If we run <code>calkit status</code> we see there are some differences, so we run <code>calkit save -am \"Change x-axis label for reviewer 2\"</code> to get those saved and backed up. If we go visit the project on the Calkit website, we see our figure is up-to-date and has the requested changes in the legend. Reviewer 2 will be so happy \ud83d\ude00</p> <p></p>"},{"location":"tutorials/openfoam/#conclusions-and-next-steps","title":"Conclusions and next steps","text":"<p>We created a project that runs OpenFOAM simulations reproducibly, produces a figure comparing against an imported dataset, and ensures these are kept in version control and backed up to the cloud. This is of course a simplified example for demonstration, but you could imagine expanding the pipeline to include more operations, such as:</p> <ul> <li>Running a mesh independence study.</li> <li>Building a LaTeX document as a report or paper.</li> </ul> <p>Since DVC pipelines can run any arbitrary command, you're not locked into a specific language or framework. You could run shell scripts, MATLAB programs, etc., all in a single pipeline.</p> <p>See this project up on Calkit and GitHub.</p>"},{"location":"tutorials/procedures/","title":"Defining and executing procedures","text":""},{"location":"tutorials/procedures/#defining-and-executing-procedures","title":"Defining and executing procedures","text":"<p>Not everything can be automated... yet. Sometimes we need to perform manual procedures as part of a research protocol, e.g., to collect data during an experiment, or simply to get the equipment ready. These procedures can be potentially complex, and in order to be reproducible, they should be thoroughly documented and tracked as they're being carried out.</p> <p>To help make this easier, we can define and execute procedures with Calkit. This will allow you to define it ahead of time and not need to waste time, e.g., during an experiment, figuring out what step you're on.</p>"},{"location":"tutorials/procedures/#defining","title":"Defining","text":"<p>The <code>Procedure</code> model in <code>calkit.models</code> shows the structure of a procedure. For example, we might define a procedure with 3 steps like:</p> <pre><code>title: My important procedure\ndescription: This is a manual procedure for setting up the experiment.\nsteps:\n  - summary: Turn on the machine\n    wait_after_s: 5\n  - summary: Record the temperature\n    details: &gt;\n      In the upper right hand corner of the screen you will see a temperature\n      value. Record this.\n    inputs:\n      temperature:\n        units: Degrees C\n        dtype: float\n  - summary: Turn off the machine\n    details: Press the power button.\n</code></pre> <p>We can save this anywhere, but to follow convention we will save to <code>.calkit/procedures/my-important-procedure.yaml</code>.</p> <p>In <code>calkit.yaml</code>, we can add to the <code>procedures</code> like:</p> <pre><code>procedures:\n  my-important-procedure:\n    _include: .calkit/procedures/my-important-procedure.yaml\n</code></pre> <p>Here we use an <code>_include</code> key to reference the other file to help keep <code>calkit.yaml</code> easier to read.</p>"},{"location":"tutorials/procedures/#executing","title":"Executing","text":"<p>If we run <code>calkit xproc my-important-procedure</code> from the command line, our procedure will start. We will be prompted to perform the first step and press enter to confirm.</p> <p>After confirming we've completed the first step, Calkit is going to wait 5 seconds before asking us to perform the next step, since we defined the <code>wait_after_s</code> attribute. While we wait, we'll see a countdown timer, then once time is up, we'll be prompted to complete the next step.</p> <p>The second step (numbered as step 1, since we're zero-indexed) defines an input called <code>temperature</code>. The user will be prompted to enter a value, and in this case it will need to be a valid <code>float</code>.</p> <p></p>"},{"location":"tutorials/procedures/#logging","title":"Logging","text":"<p>As we run through the procedure, Calkit will be logging each step and committing to the Git repo. These logs will be saved as CSVs with paths like <code>.calkit/procedure-runs/{procedure_name}/{start_date_time}.csv</code>. The CSV file will have columns indicating what step number was performed, when it was started, when it was finished, and will have a column for each input defined, if applicable.</p> <p>These logs can be read later for further analysis and/or visualization.</p>"},{"location":"tutorials/procedures/#executing-as-part-of-the-pipeline","title":"Executing as part of the pipeline","text":"<p>Let's imagine we want to execute a procedure to collect some data and then generate a plot of that data. We can define this in our DVC pipeline so we know if/when the procedure has been run, and if the plot need to be remade.</p> <pre><code>stages:\n  run-proc:\n    cmd: calkit xproc my-important-procedure\n    outs:\n      - .calkit/procedure-runs/my-important-procedure:\n          cache: false # Track this in Git, not DVC\n          persist: true # Don't delete existing outputs\n  plot-data:\n    cmd: python scripts/plot-data.py\n    deps:\n      - .calkit/procedure-runs/my-important-procedure\n    outs:\n      - figures/my-plot.png\n</code></pre> <p>With this pipeline, when we execute <code>calkit run</code>, if our procedure has never been executed, it will begin right then. After completion, our <code>plot-data</code> stage will run.</p> <p>If the procedure has been run once, but we want to run it again, we can use the <code>-f</code> flag to force it to be called, even though we already data present in <code>.calkit/procedure-runs/my-important-procedure</code>. After that, our <code>plot-data</code> stage will run since the procedure log folder was defined as its input. So again, with one command we can ensure all of our inputs and outputs are consistent.</p>"}]}